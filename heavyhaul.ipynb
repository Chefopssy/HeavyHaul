{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cd79515-1d31-4c53-84fa-3e37819e85b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset created and saved as 'shipment_logistics_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Helper functions\n",
    "def random_coord():\n",
    "    return f\"{round(random.uniform(-90,90),4)},{round(random.uniform(-180,180),4)}\"\n",
    "\n",
    "def random_dim():\n",
    "    return f\"{random.randint(15,50)}x{random.randint(10,40)}x{random.randint(8,30)}\"\n",
    "\n",
    "def random_date(start, end):\n",
    "    return start + timedelta(days=random.randint(0, (end-start).days))\n",
    "\n",
    "# Constants\n",
    "num_rows = 10000\n",
    "base_date = datetime(2025, 5, 1)\n",
    "\n",
    "origins = [\"New York\", \"Chicago\", \"Miami\", \"Boston\", \"Dallas\", \"Atlanta\", \"San Diego\", \"Philadelphia\", \"Houston\", \"Detroit\"]\n",
    "destinations = [\"Los Angeles\", \"Houston\", \"Seattle\", \"San Francisco\", \"Denver\", \"Miami\", \"Las Vegas\", \"Phoenix\", \"Austin\", \"Orlando\"]\n",
    "carriers = [\"FedEx\", \"UPS\", \"DHL\"]\n",
    "shipment_types = [\"Standard\", \"Express\", \"Fragile\"]\n",
    "statuses = [\"Delivered\", \"Delayed\", \"Disrupted\"]\n",
    "delay_reasons = [\"\", \"Weather Delay\", \"Traffic Jam\", \"Strike\", \"Vehicle Breakdown\", \"Customs Issue\"]\n",
    "weather_types = [\"Clear\", \"Rain\", \"Storm\", \"Cloudy\", \"Sunny\"]\n",
    "traffic_levels = [\"Low\", \"Moderate\", \"High\", \"Severe\"]\n",
    "events = [\"None\", \"Holiday\", \"Strike\"]\n",
    "maintenance_types = [\"Oil Change\", \"Brake Check\", \"Inspection\", \"Battery Replacement\", \"Filter Change\"]\n",
    "image_labels = [\"OK\", \"Damaged\"]\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Data generation\n",
    "for i in range(num_rows):\n",
    "    sid = 1001 + i\n",
    "    oid = 5001 + i\n",
    "    custid = f\"C{random.randint(100,999)}\"\n",
    "    origin = random.choice(origins)\n",
    "    dest = random.choice(destinations)\n",
    "    carrier = random.choice(carriers)\n",
    "    routeid = f\"R{random.randint(1,20)}\"\n",
    "    planned_ship = random_date(base_date, base_date + timedelta(days=30))\n",
    "    actual_ship = planned_ship + timedelta(days=random.choice([0,0,0,1]))\n",
    "    planned_del = planned_ship + timedelta(days=random.randint(1,7))\n",
    "    actual_del = planned_del + timedelta(days=random.choice([0,0,0,0,1,2]))\n",
    "    status = random.choices(statuses, weights=[70,20,10])[0]\n",
    "    stype = random.choice(shipment_types)\n",
    "    weight = round(random.uniform(50,350),1)\n",
    "    vol = round(weight/100 + random.uniform(0.5,3),1)\n",
    "    dims = random_dim()\n",
    "    delayflag = 1 if status != \"Delivered\" else 0\n",
    "    disruption = random.choice(delay_reasons) if status == \"Disrupted\" else \"\"\n",
    "    weather = random.choice(weather_types)\n",
    "    traffic = random.choice(traffic_levels)\n",
    "    event = random.choice(events)\n",
    "    vid = f\"V{random.randint(1,30):03d}\"\n",
    "    engine_hours = random.randint(1000,8000)\n",
    "    mileage = random.randint(20000,150000)\n",
    "    fuel = round(random.uniform(4.5,12.0),1)\n",
    "    gps = f\"{random_coord()};{random_coord()}\"\n",
    "    temp = random.randint(15,30)\n",
    "    vibr = round(random.uniform(0.01,0.1),2)\n",
    "    maint_date = planned_ship - timedelta(days=random.randint(1,30))\n",
    "    maint_type = random.choice(maintenance_types)\n",
    "    breakdown = random.choices([0,1],weights=[97,3])[0]\n",
    "    pkg_img = f\"package_{sid}_before.jpg\"\n",
    "    img_label = random.choice(image_labels)\n",
    "    incident = random.choices([0,1],weights=[95,5])[0]\n",
    "    anomaly = random.choices([0,1],weights=[96,4])[0]\n",
    "\n",
    "    row = [\n",
    "        sid, oid, custid, origin, dest, carrier, routeid,\n",
    "        planned_ship.date(), actual_ship.date(), planned_del.date(), actual_del.date(), status, stype, weight, vol, dims, delayflag,\n",
    "        disruption, weather, traffic, event, vid, engine_hours, mileage, fuel, gps, temp, vibr, maint_date.date(), maint_type, breakdown,\n",
    "        pkg_img, img_label, incident, anomaly\n",
    "    ]\n",
    "    rows.append(row)\n",
    "\n",
    "# Define column names\n",
    "columns = [\n",
    "    \"ShipmentID\",\"OrderID\",\"CustomerID\",\"Origin\",\"Destination\",\"Carrier\",\"RouteID\",\"PlannedShipDate\",\"ActualShipDate\",\n",
    "    \"PlannedDeliveryDate\",\"ActualDeliveryDate\",\"Status\",\"ShipmentType\",\"Weight\",\"Volume\",\"Dimensions\",\"DelayFlag\",\n",
    "    \"DisruptionReason\",\"Weather\",\"Traffic\",\"PublicEvent\",\"VehicleID\",\"EngineHours\",\"Mileage\",\"FuelConsumption\",\"GPS\",\n",
    "    \"Temperature\",\"Vibration\",\"MaintenanceDate\",\"MaintenanceType\",\"BreakdownFlag\",\"PackageImage\",\"ImageLabel\",\"IncidentFlag\",\"AnomalyFlag\"\n",
    "]\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "df.to_csv(\"shipment_logistics_dataset.csv\", index=False)\n",
    "print(\"✅ Dataset created and saved as 'shipment_logistics_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caa1450d-7cce-498c-91a7-e64eed82a383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ShipmentID</th>\n",
       "      <th>OrderID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Carrier</th>\n",
       "      <th>RouteID</th>\n",
       "      <th>PlannedShipDate</th>\n",
       "      <th>ActualShipDate</th>\n",
       "      <th>PlannedDeliveryDate</th>\n",
       "      <th>...</th>\n",
       "      <th>GPS</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Vibration</th>\n",
       "      <th>MaintenanceDate</th>\n",
       "      <th>MaintenanceType</th>\n",
       "      <th>BreakdownFlag</th>\n",
       "      <th>PackageImage</th>\n",
       "      <th>ImageLabel</th>\n",
       "      <th>IncidentFlag</th>\n",
       "      <th>AnomalyFlag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>5001</td>\n",
       "      <td>C525</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>UPS</td>\n",
       "      <td>R11</td>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>2025-05-26</td>\n",
       "      <td>...</td>\n",
       "      <td>55.0719,-132.8542;-66.4432,90.3484</td>\n",
       "      <td>21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>2025-05-06</td>\n",
       "      <td>Battery Replacement</td>\n",
       "      <td>0</td>\n",
       "      <td>package_1001_before.jpg</td>\n",
       "      <td>OK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>5002</td>\n",
       "      <td>C805</td>\n",
       "      <td>Miami</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>FedEx</td>\n",
       "      <td>R13</td>\n",
       "      <td>2025-05-11</td>\n",
       "      <td>2025-05-12</td>\n",
       "      <td>2025-05-18</td>\n",
       "      <td>...</td>\n",
       "      <td>-72.4671,-92.4337;-13.9777,-170.9333</td>\n",
       "      <td>26</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2025-04-24</td>\n",
       "      <td>Oil Change</td>\n",
       "      <td>0</td>\n",
       "      <td>package_1002_before.jpg</td>\n",
       "      <td>OK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>5003</td>\n",
       "      <td>C926</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>FedEx</td>\n",
       "      <td>R18</td>\n",
       "      <td>2025-05-22</td>\n",
       "      <td>2025-05-23</td>\n",
       "      <td>2025-05-28</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.999,-87.9321;-4.5549,33.0992</td>\n",
       "      <td>17</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2025-05-07</td>\n",
       "      <td>Filter Change</td>\n",
       "      <td>0</td>\n",
       "      <td>package_1003_before.jpg</td>\n",
       "      <td>Damaged</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>5004</td>\n",
       "      <td>C486</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>DHL</td>\n",
       "      <td>R7</td>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>2025-05-27</td>\n",
       "      <td>...</td>\n",
       "      <td>6.7314,-133.5211;-14.1871,122.965</td>\n",
       "      <td>18</td>\n",
       "      <td>0.09</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>Battery Replacement</td>\n",
       "      <td>0</td>\n",
       "      <td>package_1004_before.jpg</td>\n",
       "      <td>Damaged</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>5005</td>\n",
       "      <td>C923</td>\n",
       "      <td>Boston</td>\n",
       "      <td>Denver</td>\n",
       "      <td>FedEx</td>\n",
       "      <td>R17</td>\n",
       "      <td>2025-05-17</td>\n",
       "      <td>2025-05-17</td>\n",
       "      <td>2025-05-23</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.5173,16.6943;72.8763,-124.9511</td>\n",
       "      <td>25</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2025-04-26</td>\n",
       "      <td>Battery Replacement</td>\n",
       "      <td>0</td>\n",
       "      <td>package_1005_before.jpg</td>\n",
       "      <td>OK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>10996</td>\n",
       "      <td>14996</td>\n",
       "      <td>C467</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Houston</td>\n",
       "      <td>UPS</td>\n",
       "      <td>R16</td>\n",
       "      <td>2025-05-14</td>\n",
       "      <td>2025-05-15</td>\n",
       "      <td>2025-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>6.631,-36.3833;-47.1611,172.091</td>\n",
       "      <td>29</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2025-05-05</td>\n",
       "      <td>Brake Check</td>\n",
       "      <td>0</td>\n",
       "      <td>package_10996_before.jpg</td>\n",
       "      <td>OK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>10997</td>\n",
       "      <td>14997</td>\n",
       "      <td>C638</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>DHL</td>\n",
       "      <td>R11</td>\n",
       "      <td>2025-05-11</td>\n",
       "      <td>2025-05-11</td>\n",
       "      <td>2025-05-16</td>\n",
       "      <td>...</td>\n",
       "      <td>7.1184,-92.9857;13.3409,131.0448</td>\n",
       "      <td>17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>Battery Replacement</td>\n",
       "      <td>0</td>\n",
       "      <td>package_10997_before.jpg</td>\n",
       "      <td>OK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>10998</td>\n",
       "      <td>14998</td>\n",
       "      <td>C376</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Denver</td>\n",
       "      <td>DHL</td>\n",
       "      <td>R9</td>\n",
       "      <td>2025-05-06</td>\n",
       "      <td>2025-05-07</td>\n",
       "      <td>2025-05-08</td>\n",
       "      <td>...</td>\n",
       "      <td>77.3513,-64.8927;-63.182,-14.3878</td>\n",
       "      <td>28</td>\n",
       "      <td>0.09</td>\n",
       "      <td>2025-04-11</td>\n",
       "      <td>Oil Change</td>\n",
       "      <td>0</td>\n",
       "      <td>package_10998_before.jpg</td>\n",
       "      <td>Damaged</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>10999</td>\n",
       "      <td>14999</td>\n",
       "      <td>C782</td>\n",
       "      <td>Detroit</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>FedEx</td>\n",
       "      <td>R14</td>\n",
       "      <td>2025-05-30</td>\n",
       "      <td>2025-05-30</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-26.0669,-115.4695;51.2226,156.5128</td>\n",
       "      <td>29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2025-05-06</td>\n",
       "      <td>Oil Change</td>\n",
       "      <td>0</td>\n",
       "      <td>package_10999_before.jpg</td>\n",
       "      <td>OK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>11000</td>\n",
       "      <td>15000</td>\n",
       "      <td>C489</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Denver</td>\n",
       "      <td>DHL</td>\n",
       "      <td>R16</td>\n",
       "      <td>2025-05-15</td>\n",
       "      <td>2025-05-15</td>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>...</td>\n",
       "      <td>29.6046,30.9789;56.8689,-132.8548</td>\n",
       "      <td>22</td>\n",
       "      <td>0.09</td>\n",
       "      <td>2025-05-10</td>\n",
       "      <td>Inspection</td>\n",
       "      <td>0</td>\n",
       "      <td>package_11000_before.jpg</td>\n",
       "      <td>OK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ShipmentID  OrderID CustomerID     Origin  Destination Carrier RouteID  \\\n",
       "0           1001     5001       C525    Atlanta      Phoenix     UPS     R11   \n",
       "1           1002     5002       C805      Miami  Los Angeles   FedEx     R13   \n",
       "2           1003     5003       C926  San Diego  Los Angeles   FedEx     R18   \n",
       "3           1004     5004       C486    Houston      Phoenix     DHL      R7   \n",
       "4           1005     5005       C923     Boston       Denver   FedEx     R17   \n",
       "...          ...      ...        ...        ...          ...     ...     ...   \n",
       "9995       10996    14996       C467    Chicago      Houston     UPS     R16   \n",
       "9996       10997    14997       C638    Houston  Los Angeles     DHL     R11   \n",
       "9997       10998    14998       C376    Houston       Denver     DHL      R9   \n",
       "9998       10999    14999       C782    Detroit      Phoenix   FedEx     R14   \n",
       "9999       11000    15000       C489    Houston       Denver     DHL     R16   \n",
       "\n",
       "     PlannedShipDate ActualShipDate PlannedDeliveryDate  ...  \\\n",
       "0         2025-05-20     2025-05-20          2025-05-26  ...   \n",
       "1         2025-05-11     2025-05-12          2025-05-18  ...   \n",
       "2         2025-05-22     2025-05-23          2025-05-28  ...   \n",
       "3         2025-05-21     2025-05-21          2025-05-27  ...   \n",
       "4         2025-05-17     2025-05-17          2025-05-23  ...   \n",
       "...              ...            ...                 ...  ...   \n",
       "9995      2025-05-14     2025-05-15          2025-05-15  ...   \n",
       "9996      2025-05-11     2025-05-11          2025-05-16  ...   \n",
       "9997      2025-05-06     2025-05-07          2025-05-08  ...   \n",
       "9998      2025-05-30     2025-05-30          2025-06-05  ...   \n",
       "9999      2025-05-15     2025-05-15          2025-05-20  ...   \n",
       "\n",
       "                                       GPS Temperature Vibration  \\\n",
       "0       55.0719,-132.8542;-66.4432,90.3484          21      0.07   \n",
       "1     -72.4671,-92.4337;-13.9777,-170.9333          26      0.05   \n",
       "2          -7.999,-87.9321;-4.5549,33.0992          17      0.10   \n",
       "3        6.7314,-133.5211;-14.1871,122.965          18      0.09   \n",
       "4       -18.5173,16.6943;72.8763,-124.9511          25      0.04   \n",
       "...                                    ...         ...       ...   \n",
       "9995       6.631,-36.3833;-47.1611,172.091          29      0.05   \n",
       "9996      7.1184,-92.9857;13.3409,131.0448          17      0.04   \n",
       "9997     77.3513,-64.8927;-63.182,-14.3878          28      0.09   \n",
       "9998   -26.0669,-115.4695;51.2226,156.5128          29      0.03   \n",
       "9999     29.6046,30.9789;56.8689,-132.8548          22      0.09   \n",
       "\n",
       "      MaintenanceDate      MaintenanceType BreakdownFlag  \\\n",
       "0          2025-05-06  Battery Replacement             0   \n",
       "1          2025-04-24           Oil Change             0   \n",
       "2          2025-05-07        Filter Change             0   \n",
       "3          2025-05-01  Battery Replacement             0   \n",
       "4          2025-04-26  Battery Replacement             0   \n",
       "...               ...                  ...           ...   \n",
       "9995       2025-05-05          Brake Check             0   \n",
       "9996       2025-04-30  Battery Replacement             0   \n",
       "9997       2025-04-11           Oil Change             0   \n",
       "9998       2025-05-06           Oil Change             0   \n",
       "9999       2025-05-10           Inspection             0   \n",
       "\n",
       "                  PackageImage ImageLabel IncidentFlag AnomalyFlag  \n",
       "0      package_1001_before.jpg         OK            0           0  \n",
       "1      package_1002_before.jpg         OK            0           0  \n",
       "2      package_1003_before.jpg    Damaged            0           0  \n",
       "3      package_1004_before.jpg    Damaged            0           0  \n",
       "4      package_1005_before.jpg         OK            0           0  \n",
       "...                        ...        ...          ...         ...  \n",
       "9995  package_10996_before.jpg         OK            0           0  \n",
       "9996  package_10997_before.jpg         OK            0           0  \n",
       "9997  package_10998_before.jpg    Damaged            0           0  \n",
       "9998  package_10999_before.jpg         OK            0           0  \n",
       "9999  package_11000_before.jpg         OK            0           0  \n",
       "\n",
       "[10000 rows x 35 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a2fe29-4874-4ef2-9b3b-f21109d94e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 35 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   ShipmentID           10000 non-null  int64  \n",
      " 1   OrderID              10000 non-null  int64  \n",
      " 2   CustomerID           10000 non-null  object \n",
      " 3   Origin               10000 non-null  object \n",
      " 4   Destination          10000 non-null  object \n",
      " 5   Carrier              10000 non-null  object \n",
      " 6   RouteID              10000 non-null  object \n",
      " 7   PlannedShipDate      10000 non-null  object \n",
      " 8   ActualShipDate       10000 non-null  object \n",
      " 9   PlannedDeliveryDate  10000 non-null  object \n",
      " 10  ActualDeliveryDate   10000 non-null  object \n",
      " 11  Status               10000 non-null  object \n",
      " 12  ShipmentType         10000 non-null  object \n",
      " 13  Weight               10000 non-null  float64\n",
      " 14  Volume               10000 non-null  float64\n",
      " 15  Dimensions           10000 non-null  object \n",
      " 16  DelayFlag            10000 non-null  int64  \n",
      " 17  DisruptionReason     10000 non-null  object \n",
      " 18  Weather              10000 non-null  object \n",
      " 19  Traffic              10000 non-null  object \n",
      " 20  PublicEvent          10000 non-null  object \n",
      " 21  VehicleID            10000 non-null  object \n",
      " 22  EngineHours          10000 non-null  int64  \n",
      " 23  Mileage              10000 non-null  int64  \n",
      " 24  FuelConsumption      10000 non-null  float64\n",
      " 25  GPS                  10000 non-null  object \n",
      " 26  Temperature          10000 non-null  int64  \n",
      " 27  Vibration            10000 non-null  float64\n",
      " 28  MaintenanceDate      10000 non-null  object \n",
      " 29  MaintenanceType      10000 non-null  object \n",
      " 30  BreakdownFlag        10000 non-null  int64  \n",
      " 31  PackageImage         10000 non-null  object \n",
      " 32  ImageLabel           10000 non-null  object \n",
      " 33  IncidentFlag         10000 non-null  int64  \n",
      " 34  AnomalyFlag          10000 non-null  int64  \n",
      "dtypes: float64(4), int64(9), object(22)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b3c310-6854-42b2-b11a-80aebb39090d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShipmentID             0\n",
       "OrderID                0\n",
       "CustomerID             0\n",
       "Origin                 0\n",
       "Destination            0\n",
       "Carrier                0\n",
       "RouteID                0\n",
       "PlannedShipDate        0\n",
       "ActualShipDate         0\n",
       "PlannedDeliveryDate    0\n",
       "ActualDeliveryDate     0\n",
       "Status                 0\n",
       "ShipmentType           0\n",
       "Weight                 0\n",
       "Volume                 0\n",
       "Dimensions             0\n",
       "DelayFlag              0\n",
       "DisruptionReason       0\n",
       "Weather                0\n",
       "Traffic                0\n",
       "PublicEvent            0\n",
       "VehicleID              0\n",
       "EngineHours            0\n",
       "Mileage                0\n",
       "FuelConsumption        0\n",
       "GPS                    0\n",
       "Temperature            0\n",
       "Vibration              0\n",
       "MaintenanceDate        0\n",
       "MaintenanceType        0\n",
       "BreakdownFlag          0\n",
       "PackageImage           0\n",
       "ImageLabel             0\n",
       "IncidentFlag           0\n",
       "AnomalyFlag            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed9adfa8-b37f-4617-b23e-42df265b3f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: task1_to_3_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"shipment_logistics_dataset.csv\", parse_dates=[\"ActualShipDate\", \"ActualDeliveryDate\"])\n",
    "df[\"DeliveryTimeDays\"] = (df[\"ActualDeliveryDate\"] - df[\"ActualShipDate\"]).dt.days\n",
    "\n",
    "shipment_df = df[[\n",
    "    \"Origin\", \"Destination\", \"Carrier\", \"ShipmentType\",\n",
    "    \"Weight\", \"Volume\", \"Dimensions\",\n",
    "    \"PlannedShipDate\", \"ActualShipDate\", \"PlannedDeliveryDate\", \"ActualDeliveryDate\",\n",
    "    \"Status\", \"DeliveryTimeDays\"\n",
    "]].dropna(subset=[\"DeliveryTimeDays\"])\n",
    "\n",
    "shipment_df.to_csv(\"task1_to_3_dataset.csv\", index=False)\n",
    "print(\"✅ Saved: task1_to_3_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f99bb659-bf66-42e5-a534-845b9eb09750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task 1 complete: Preprocessed features ready for modeling.\n"
     ]
    }
   ],
   "source": [
    "# TASK 1 — Data Preprocessing\n",
    "# Author: Opemipo Oreoluwa\n",
    "# Purpose: Prepare features for shipment delivery prediction.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load task-specific dataset\n",
    "df = pd.read_csv(\"task1_to_3_dataset.csv\")\n",
    "\n",
    "# Define input and target\n",
    "X = df[[\"Origin\", \"Destination\", \"Carrier\", \"ShipmentType\", \"Weight\", \"Volume\", \"Dimensions\"]]\n",
    "y = df[\"DeliveryTimeDays\"]\n",
    "\n",
    "# Define column types\n",
    "categorical = [\"Origin\", \"Destination\", \"Carrier\", \"ShipmentType\"]\n",
    "numerical = [\"Weight\", \"Volume\"]\n",
    "\n",
    "# Build preprocessing pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), numerical),\n",
    "    \n",
    "    (\"cat\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]), categorical)\n",
    "])\n",
    "\n",
    "# Fit & transform the data\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "print(\"✅ Task 1 complete: Preprocessed features ready for modeling.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78bdc35b-3561-4234-99ea-ca55556016da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task 2 complete — MAE: 1.8587480270235202 | R²: -0.004157442984270121\n"
     ]
    }
   ],
   "source": [
    "# TASK 2 — Train Linear Regression\n",
    "# Author: Opemipo Oreoluwa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"✅ Task 2 complete — MAE:\", mean_absolute_error(y_test, y_pred), \"| R²:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc64b38a-3415-4955-8cea-177072a5f4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 10.9786 - mae: 2.6902\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 4.8367 - mae: 1.8508\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 4.8930 - mae: 1.8643\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 4.8564 - mae: 1.8728\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.8700 - mae: 1.8645\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 4.7732 - mae: 1.8411\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.7664 - mae: 1.8442\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 4.8190 - mae: 1.8636\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 4.7474 - mae: 1.8413\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.8405 - mae: 1.8599\n",
      "✅ Task 3 complete — Neural network trained.\n"
     ]
    }
   ],
   "source": [
    "# TASK 3 — Neural Network (TensorFlow)\n",
    "# Author: Opemipo Oreoluwa\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use numeric features only\n",
    "X = df[[\"Weight\", \"Volume\"]]\n",
    "y = df[\"DeliveryTimeDays\"]\n",
    "\n",
    "# Scale and split\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=16)\n",
    "print(\"✅ Task 3 complete — Neural network trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8e123ed-c16f-4c1f-91fd-529272c278f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TASK 4 — Disruption Prediction with PyTorch\n",
    "# # Author: Opemipo Oreoluwa\n",
    "\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Load dataset\n",
    "# df1 = pd.read_csv(\"task4_disruption_dataset.csv\")\n",
    "\n",
    "# # Encode categorical features\n",
    "# for col in [\"Weather\", \"Traffic\", \"PublicEvent\"]:\n",
    "#     df1[col] = LabelEncoder().fit_transform(df1[col])\n",
    "\n",
    "# X = df1[[\"Weight\", \"Volume\", \"Weather\", \"Traffic\", \"PublicEvent\"]].values\n",
    "# y = df1[\"DisruptionFlag\"].values.reshape(-1, 1)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# # Build PyTorch model\n",
    "# class DisruptionModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(5, 16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(16, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# model = DisruptionModel()\n",
    "# loss_fn = nn.BCELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# # Train model\n",
    "# for epoch in range(20):\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(X_train)\n",
    "#     loss = loss_fn(output, y_train)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if epoch % 5 == 0:\n",
    "#         print(f\"Epoch {epoch} — Loss: {loss.item():.4f}\")\n",
    "\n",
    "# print(\"✅ Task 4 complete — PyTorch model trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ff2db1b-a3c1-4540-99cd-abb45d38ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df1 = pd.read_csv(\"shipment_logistics_dataset.csv\")\n",
    "# df1[\"DisruptionFlag\"] = (df1[\"Status\"] == \"Disrupted\").astype(int)\n",
    "\n",
    "# disruption_df = df[[\n",
    "#     \"Weight\", \"Volume\", \"Weather\", \"Traffic\", \"PublicEvent\",\n",
    "#     \"DisruptionReason\", \"DelayFlag\", \"DisruptionFlag\"\n",
    "# ]].dropna()\n",
    "\n",
    "# disruption_df.to_csv(\"task4_disruption_dataset.csv\", index=False)\n",
    "# print(\"✅ Saved: task4_disruption_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ab81e5e-a6b9-417e-8233-d90d01d5e93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ External weather_data.csv created.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generate dates\n",
    "dates = pd.date_range(start=\"2025-05-01\", periods=60).date\n",
    "routes = [f\"R{i}\" for i in range(1, 21)]\n",
    "weathers = [\"Clear\", \"Rain\", \"Storm\", \"Cloudy\", \"Sunny\"]\n",
    "\n",
    "# Create weather dataset (date x route)\n",
    "data = []\n",
    "for date in dates:\n",
    "    for route in routes:\n",
    "        data.append({\n",
    "            \"date\": date,\n",
    "            \"route\": route,\n",
    "            \"weather\": random.choice(weathers)\n",
    "        })\n",
    "\n",
    "weather_df = pd.DataFrame(data)\n",
    "weather_df.to_csv(\"weather_data.csv\", index=False)\n",
    "print(\"✅ External weather_data.csv created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9216cf48-e79a-4d1a-aa61-6794670884a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged dataset ready: task4_merged_disruption_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load shipment + weather data\n",
    "df = pd.read_csv(\"shipment_logistics_dataset.csv\", parse_dates=[\"ActualShipDate\"])\n",
    "weather = pd.read_csv(\"weather_data.csv\")\n",
    "\n",
    "# Convert to date only\n",
    "df[\"ActualShipDate\"] = df[\"ActualShipDate\"].dt.date\n",
    "\n",
    "# Add disruption flag\n",
    "df[\"DisruptionFlag\"] = (df[\"Status\"] == \"Disrupted\").astype(int)\n",
    "df[\"Route\"] = df[\"RouteID\"]\n",
    "\n",
    "# Merge on route + date\n",
    "merged_df = pd.merge(df, weather, left_on=[\"ActualShipDate\", \"Route\"], right_on=[\"date\", \"route\"])\n",
    "\n",
    "# Encode weather\n",
    "merged_df[\"weather_encoded\"] = LabelEncoder().fit_transform(merged_df[\"weather\"])\n",
    "\n",
    "# Save merged dataset for reuse\n",
    "merged_df.to_csv(\"task4_merged_disruption_dataset.csv\", index=False)\n",
    "print(\"✅ Merged dataset ready: task4_merged_disruption_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8bfdc0d-d74d-4728-b867-4ccde9269591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Final PyTorch Classifier Code — Aligned with Sample Task\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load merged dataset\n",
    "# df = pd.read_csv(\"task4_merged_disruption_dataset.csv\")\n",
    "\n",
    "# # Feature Selection: route (encoded), distance proxy (Volume), weather_encoded\n",
    "# df[\"route_encoded\"] = LabelEncoder().fit_transform(df[\"Route\"])\n",
    "# X = df[[\"route_encoded\", \"Volume\", \"weather_encoded\"]].values\n",
    "# y = df[\"DisruptionFlag\"].values.astype(float)\n",
    "\n",
    "# # Split + convert to tensors\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "# X_test  = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_train = torch.tensor(y_train.reshape(-1,1), dtype=torch.float32)\n",
    "# y_test  = torch.tensor(y_test.reshape(-1,1), dtype=torch.float32)\n",
    "\n",
    "# # Define model\n",
    "# class DisruptionPredictor(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(3, 64)\n",
    "#         self.fc2 = nn.Linear(64, 32)\n",
    "#         self.fc3 = nn.Linear(32, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         return torch.sigmoid(self.fc3(x))\n",
    "\n",
    "# model = DisruptionPredictor()\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Train model\n",
    "# for epoch in range(1, 101):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(X_train)\n",
    "#     loss = criterion(outputs, y_train)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# # Evaluate\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(X_test)\n",
    "#     predicted = (outputs > 0.5).float()\n",
    "#     accuracy = (predicted == y_test).float().mean()\n",
    "#     print(f\"✅ Task 4 Complete — Accuracy: {accuracy.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99090ccb-592e-496a-9679-0eadcbe8a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TASK 4 — Predicting Supply Chain Disruptions with External Weather Data using PyTorch\n",
    "# # Author: Opemipo Oreoluwa\n",
    "\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import random\n",
    "# from datetime import datetime, timedelta\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 1: Generate External Weather Dataset\n",
    "# # -------------------------------\n",
    "# routes = [f\"R{i}\" for i in range(1, 21)]\n",
    "# dates = pd.date_range(start=\"2025-05-01\", periods=60).date\n",
    "# weathers = [\"Clear\", \"Rain\", \"Storm\", \"Cloudy\", \"Sunny\"]\n",
    "\n",
    "# weather_data = []\n",
    "# for date in dates:\n",
    "#     for route in routes:\n",
    "#         weather_data.append({\n",
    "#             \"date\": date,\n",
    "#             \"route\": route,\n",
    "#             \"weather\": random.choice(weathers)\n",
    "#         })\n",
    "\n",
    "# weather_df = pd.DataFrame(weather_data)\n",
    "# weather_df.to_csv(\"weather_data.csv\", index=False)\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 2: Load & Prepare Shipment Dataset\n",
    "# # -------------------------------\n",
    "# df = pd.read_csv(\"shipment_logistics_dataset.csv\", parse_dates=[\"ActualShipDate\"])\n",
    "# df[\"ActualShipDate\"] = df[\"ActualShipDate\"].dt.date\n",
    "# df[\"Route\"] = df[\"RouteID\"]\n",
    "# df[\"DisruptionFlag\"] = (df[\"Status\"] == \"Disrupted\").astype(int)\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 3: Merge Shipment + Weather Data\n",
    "# # -------------------------------\n",
    "# weather = pd.read_csv(\"weather_data.csv\")\n",
    "# merged_df = pd.merge(df, weather, left_on=[\"ActualShipDate\", \"Route\"], right_on=[\"date\", \"route\"])\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 4: Encode & Prepare for PyTorch\n",
    "# # -------------------------------\n",
    "# merged_df[\"weather_encoded\"] = LabelEncoder().fit_transform(merged_df[\"weather\"])\n",
    "# merged_df[\"route_encoded\"] = LabelEncoder().fit_transform(merged_df[\"Route\"])\n",
    "\n",
    "# X = merged_df[[\"route_encoded\", \"Volume\", \"weather_encoded\"]].values\n",
    "# y = merged_df[\"DisruptionFlag\"].values.astype(float)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "# X_test  = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_train = torch.tensor(y_train.reshape(-1,1), dtype=torch.float32)\n",
    "# y_test  = torch.tensor(y_test.reshape(-1,1), dtype=torch.float32)\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 5: Define PyTorch Model\n",
    "# # -------------------------------\n",
    "# class DisruptionPredictor(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(3, 64)\n",
    "#         self.fc2 = nn.Linear(64, 32)\n",
    "#         self.fc3 = nn.Linear(32, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         return torch.sigmoid(self.fc3(x))\n",
    "\n",
    "# model = DisruptionPredictor()\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 6: Train Model\n",
    "# # -------------------------------\n",
    "# for epoch in range(1, 101):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(X_train)\n",
    "#     loss = criterion(outputs, y_train)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f\"Epoch {epoch} — Loss: {loss.item():.4f}\")\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 7: Evaluate\n",
    "# # -------------------------------\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(X_test)\n",
    "#     predicted = (outputs > 0.5).float()\n",
    "#     accuracy = (predicted == y_test).float().mean()\n",
    "#     print(f\"\\n✅ Task 4 Complete — Disruption Prediction Accuracy: {accuracy.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bc747cd-1665-4d59-901f-339f2a0ea86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Unique Routes in shipment: ['R1', 'R10', 'R11', 'R12', 'R13']\n",
      "🧪 Unique Routes in weather:  ['R1', 'R10', 'R11', 'R12', 'R13']\n",
      "\n",
      "🧪 Sample Dates in shipment: [datetime.date(2025, 5, 1), datetime.date(2025, 5, 2), datetime.date(2025, 5, 3), datetime.date(2025, 5, 4), datetime.date(2025, 5, 5)]\n",
      "🧪 Sample Dates in weather:  [datetime.date(2025, 5, 1), datetime.date(2025, 5, 2), datetime.date(2025, 5, 3), datetime.date(2025, 5, 4), datetime.date(2025, 5, 5)]\n",
      "\n",
      "🔍 Rows in merged_df: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both datasets\n",
    "shipment = pd.read_csv(\"shipment_logistics_dataset.csv\", parse_dates=[\"ActualShipDate\"])\n",
    "weather = pd.read_csv(\"weather_data.csv\")\n",
    "\n",
    "# Convert to date only\n",
    "shipment[\"ActualShipDate\"] = shipment[\"ActualShipDate\"].dt.date\n",
    "shipment[\"Route\"] = shipment[\"RouteID\"]\n",
    "\n",
    "# Check merge keys\n",
    "print(\"🧪 Unique Routes in shipment:\", sorted(shipment[\"Route\"].unique())[:5])\n",
    "print(\"🧪 Unique Routes in weather: \", sorted(weather[\"route\"].unique())[:5])\n",
    "\n",
    "print(\"\\n🧪 Sample Dates in shipment:\", sorted(shipment[\"ActualShipDate\"].unique())[:5])\n",
    "print(\"🧪 Sample Dates in weather: \", sorted(pd.to_datetime(weather[\"date\"]).dt.date.unique())[:5])\n",
    "\n",
    "# Try merging\n",
    "merged_df = pd.merge(shipment, weather, left_on=[\"ActualShipDate\", \"Route\"], right_on=[\"date\", \"route\"])\n",
    "print(f\"\\n🔍 Rows in merged_df: {merged_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faa4d186-c9eb-4e74-be8c-ca3102d7bf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 — Loss: 0.3532\n",
      "Epoch 20 — Loss: 0.3523\n",
      "Epoch 30 — Loss: 0.3396\n",
      "Epoch 40 — Loss: 0.3368\n",
      "Epoch 50 — Loss: 0.3356\n",
      "Epoch 60 — Loss: 0.3350\n",
      "Epoch 70 — Loss: 0.3344\n",
      "Epoch 80 — Loss: 0.3339\n",
      "Epoch 90 — Loss: 0.3333\n",
      "Epoch 100 — Loss: 0.3328\n",
      "\n",
      "✅ Final Task 4 Complete — Disruption Prediction Accuracy: 0.8955\n"
     ]
    }
   ],
   "source": [
    "# ✅ Generate External Weather Data Only for Actual Shipment Route-Date Combinations\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1: Load Shipment Data\n",
    "# -------------------------------\n",
    "df = pd.read_csv(\"shipment_logistics_dataset.csv\", parse_dates=[\"ActualShipDate\"])\n",
    "df[\"ActualShipDate\"] = df[\"ActualShipDate\"].dt.date\n",
    "df[\"Route\"] = df[\"RouteID\"]\n",
    "df[\"DisruptionFlag\"] = (df[\"Status\"] == \"Disrupted\").astype(int)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 2: Create Matching Weather Data (Only for Shipment Routes + Dates)\n",
    "# -------------------------------\n",
    "weather_conditions = [\"Clear\", \"Rain\", \"Storm\", \"Cloudy\", \"Sunny\"]\n",
    "unique_route_dates = df[[\"ActualShipDate\", \"Route\"]].drop_duplicates()\n",
    "weather_data = []\n",
    "\n",
    "for _, row in unique_route_dates.iterrows():\n",
    "    weather_data.append({\n",
    "        \"date\": row[\"ActualShipDate\"],\n",
    "        \"route\": row[\"Route\"],\n",
    "        \"weather\": random.choice(weather_conditions)\n",
    "    })\n",
    "\n",
    "weather_df = pd.DataFrame(weather_data)\n",
    "weather_df.to_csv(\"weather_data.csv\", index=False)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 3: Merge Shipment + Weather Data\n",
    "# -------------------------------\n",
    "merged_df = pd.merge(df, weather_df, left_on=[\"ActualShipDate\", \"Route\"], right_on=[\"date\", \"route\"])\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 4: Encode & Prepare Dataset\n",
    "# -------------------------------\n",
    "merged_df[\"weather_encoded\"] = LabelEncoder().fit_transform(merged_df[\"weather\"])\n",
    "merged_df[\"route_encoded\"] = LabelEncoder().fit_transform(merged_df[\"Route\"])\n",
    "\n",
    "X = merged_df[[\"route_encoded\", \"Volume\", \"weather_encoded\"]].values\n",
    "y = merged_df[\"DisruptionFlag\"].values.astype(float)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test  = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.reshape(-1,1), dtype=torch.float32)\n",
    "y_test  = torch.tensor(y_test.reshape(-1,1), dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 5: Define & Train PyTorch Model\n",
    "# -------------------------------\n",
    "class DisruptionPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x))\n",
    "\n",
    "model = DisruptionPredictor()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch} — Loss: {loss.item():.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 6: Evaluate\n",
    "# -------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = (outputs > 0.5).float()\n",
    "    accuracy = (predicted == y_test).float().mean()\n",
    "    print(f\"\\n✅ Final Task 4 Complete — Disruption Prediction Accuracy: {accuracy.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a512764f-5f1d-4a88-a5a4-f8407774198d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step\n",
      "✅ Task 5 complete — Forecasted Avg Delivery Time for next day: 4.22 days\n"
     ]
    }
   ],
   "source": [
    "# TASK 5 — Time Series Forecasting with TensorFlow\n",
    "# Author: Opemipo Oreoluwa\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1: Load + Prepare Time Series Data\n",
    "# -------------------------------\n",
    "df = pd.read_csv(\"shipment_logistics_dataset.csv\", parse_dates=[\"ActualShipDate\", \"ActualDeliveryDate\"])\n",
    "df[\"DeliveryTimeDays\"] = (df[\"ActualDeliveryDate\"] - df[\"ActualShipDate\"]).dt.days\n",
    "df = df[[\"ActualShipDate\", \"DeliveryTimeDays\"]].dropna()\n",
    "\n",
    "# Group by date → average delivery time\n",
    "daily_df = df.groupby(\"ActualShipDate\")[\"DeliveryTimeDays\"].mean().reset_index()\n",
    "daily_df.rename(columns={\"DeliveryTimeDays\": \"AvgDeliveryTime\"}, inplace=True)\n",
    "\n",
    "# Save for record\n",
    "daily_df.to_csv(\"task5_timeseries_dataset.csv\", index=False)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 2: Normalize + Create Sequences\n",
    "# -------------------------------\n",
    "scaler = MinMaxScaler()\n",
    "scaled_series = scaler.fit_transform(daily_df[[\"AvgDeliveryTime\"]])\n",
    "\n",
    "def create_sequences(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "window_size = 5  # number of days used to predict the next\n",
    "X, y = create_sequences(scaled_series, window_size)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 3: Build & Train LSTM Model\n",
    "# -------------------------------\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=(X.shape[1], X.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "history = model.fit(X, y, epochs=50, verbose=0)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 4: Forecast Next Day\n",
    "# -------------------------------\n",
    "last_window = scaled_series[-window_size:]\n",
    "last_window = np.expand_dims(last_window, axis=0)\n",
    "pred_scaled = model.predict(last_window)\n",
    "pred_actual = scaler.inverse_transform(pred_scaled)\n",
    "\n",
    "print(f\"✅ Task 5 complete — Forecasted Avg Delivery Time for next day: {pred_actual[0][0]:.2f} days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a13a67e4-1a3f-4dae-92c4-84015e3632f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TASK 6 — Package Image Classification with CNN\n",
    "# # Author: Opemipo Oreoluwa\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "# import os\n",
    "# import shutil\n",
    "# import pandas as pd\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 1: Prepare Dummy Image Directory from CSV\n",
    "# # -------------------------------\n",
    "\n",
    "# # Create folders: package_images/OK/ and package_images/Damaged/\n",
    "# df = pd.read_csv(\"task6_image_labels_dataset.csv\")\n",
    "\n",
    "# base_dir = \"package_images\"\n",
    "# for label in df[\"ImageLabel\"].unique():\n",
    "#     os.makedirs(os.path.join(base_dir, label), exist_ok=True)\n",
    "\n",
    "# # Create dummy images with matching filenames\n",
    "# for _, row in df.iterrows():\n",
    "#     img_name, label = row[\"PackageImage\"], row[\"ImageLabel\"]\n",
    "#     img_path = os.path.join(base_dir, label, img_name)\n",
    "#     if not os.path.exists(img_path):\n",
    "#         dummy_img = Image.fromarray(np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8))\n",
    "#         dummy_img.save(img_path)\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 2: Load Dataset from Folder\n",
    "# # -------------------------------\n",
    "# img_height, img_width = 100, 100\n",
    "\n",
    "# ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#     base_dir,\n",
    "#     image_size=(img_height, img_width),\n",
    "#     batch_size=16,\n",
    "#     label_mode=\"binary\",\n",
    "#     validation_split=0.2,\n",
    "#     subset=\"training\",\n",
    "#     seed=123\n",
    "# )\n",
    "\n",
    "# val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#     base_dir,\n",
    "#     image_size=(img_height, img_width),\n",
    "#     batch_size=16,\n",
    "#     label_mode=\"binary\",\n",
    "#     validation_split=0.2,\n",
    "#     subset=\"validation\",\n",
    "#     seed=123\n",
    "# )\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 3: Build CNN Model\n",
    "# # -------------------------------\n",
    "# model = models.Sequential([\n",
    "#     layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "#     layers.Conv2D(32, (3,3), activation='relu'),\n",
    "#     layers.MaxPooling2D(),\n",
    "#     layers.Conv2D(64, (3,3), activation='relu'),\n",
    "#     layers.MaxPooling2D(),\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     layers.Dense(1, activation='sigmoid')  # binary output\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 4: Train the Model\n",
    "# # -------------------------------\n",
    "# history = model.fit(ds, validation_data=val_ds, epochs=5)\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 5: Evaluate Model\n",
    "# # -------------------------------\n",
    "# loss, acc = model.evaluate(val_ds)\n",
    "# print(f\"\\n✅ Task 6 Complete — Validation Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3059f21-f0af-4e43-8c6a-9ec5c674b7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: task6_image_labels_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"shipment_logistics_dataset.csv\")\n",
    "df[[\"PackageImage\", \"ImageLabel\"]].to_csv(\"task6_image_labels_dataset.csv\", index=False)\n",
    "print(\"✅ Saved: task6_image_labels_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "601ab2f4-38ed-4c44-8a80-c7c3b1301c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8001 images belonging to 2 classes.\n",
      "Found 1999 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\USER PC\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 2s/step - accuracy: 0.4900 - loss: 0.7981 - val_accuracy: 0.5038 - val_loss: 0.6931\n",
      "Epoch 2/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 1s/step - accuracy: 0.4919 - loss: 0.6932 - val_accuracy: 0.5038 - val_loss: 0.6931\n",
      "Epoch 3/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 1s/step - accuracy: 0.5048 - loss: 0.6931 - val_accuracy: 0.5038 - val_loss: 0.6931\n",
      "Epoch 4/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 1s/step - accuracy: 0.4998 - loss: 0.6932 - val_accuracy: 0.5038 - val_loss: 0.6931\n",
      "Epoch 5/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 1s/step - accuracy: 0.4985 - loss: 0.6932 - val_accuracy: 0.5038 - val_loss: 0.6931\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 287ms/step - accuracy: 0.5029 - loss: 0.6931\n",
      "✅ Task 6 Complete — Validation Accuracy: 0.5038\n"
     ]
    }
   ],
   "source": [
    "# TASK 6 — Image Classification for Package Inspection with CNN\n",
    "# Author: Opemipo Oreoluwa\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1: Rebuild Label File (if missing)\n",
    "# -------------------------------\n",
    "df = pd.read_csv(\"shipment_logistics_dataset.csv\")\n",
    "df[[\"PackageImage\", \"ImageLabel\"]].to_csv(\"task6_image_labels_dataset.csv\", index=False)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 2: Prepare Dummy Images Folder\n",
    "# -------------------------------\n",
    "label_df = pd.read_csv(\"task6_image_labels_dataset.csv\")\n",
    "base_dir = \"package_images\"\n",
    "\n",
    "# Clear and recreate folder structure\n",
    "if os.path.exists(base_dir):\n",
    "    shutil.rmtree(base_dir)\n",
    "\n",
    "for label in [\"OK\", \"Damaged\"]:\n",
    "    os.makedirs(os.path.join(base_dir, label), exist_ok=True)\n",
    "\n",
    "# Generate dummy images matching filenames\n",
    "for _, row in label_df.iterrows():\n",
    "    label = row[\"ImageLabel\"]\n",
    "    name = row[\"PackageImage\"]\n",
    "    img_path = os.path.join(base_dir, label, name)\n",
    "    if not os.path.exists(img_path):\n",
    "        img = Image.fromarray(np.random.randint(0, 255, (150, 150, 3), dtype=np.uint8))\n",
    "        img.save(img_path)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 3: Load Images with ImageDataGenerator\n",
    "# -------------------------------\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 4: Define CNN Model\n",
    "# -------------------------------\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 5: Train & Evaluate\n",
    "# -------------------------------\n",
    "model.fit(train_gen, epochs=5, validation_data=val_gen)\n",
    "\n",
    "loss, accuracy = model.evaluate(val_gen)\n",
    "print(f\"✅ Task 6 Complete — Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3b237709-e2ff-43f7-bc95-5ed326f1a12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8001 images belonging to 2 classes.\n",
      "Found 1999 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\USER PC\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 1s/step - accuracy: 0.4993 - loss: 0.8948 - val_accuracy: 0.5038 - val_loss: 0.6931\n",
      "Epoch 2/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 827ms/step - accuracy: 0.5112 - loss: 0.6932 - val_accuracy: 0.4962 - val_loss: 0.6932\n",
      "Epoch 3/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 1s/step - accuracy: 0.4979 - loss: 0.6932 - val_accuracy: 0.5038 - val_loss: 0.6931\n",
      "Epoch 4/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 927ms/step - accuracy: 0.5025 - loss: 0.6932 - val_accuracy: 0.5038 - val_loss: 0.6931\n",
      "Epoch 5/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 818ms/step - accuracy: 0.5059 - loss: 0.6931 - val_accuracy: 0.5038 - val_loss: 0.6931\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 199ms/step - accuracy: 0.4936 - loss: 0.6933\n",
      "✅ Task 6 Complete — Validation Accuracy: 0.5038\n",
      "✅ Task 6 model saved to models/task6_package_cnn.keras\n"
     ]
    }
   ],
   "source": [
    "# TASK 6 — Image Classification for Package Inspection with CNN\n",
    "# Author: Opemipo Oreoluwa\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# STEP 1: Rebuild Label File (if missing)\n",
    "df = pd.read_csv(\"shipment_logistics_dataset.csv\")\n",
    "df[[\"PackageImage\", \"ImageLabel\"]].to_csv(\"task6_image_labels_dataset.csv\", index=False)\n",
    "\n",
    "# STEP 2: Prepare Dummy Images Folder\n",
    "label_df = pd.read_csv(\"task6_image_labels_dataset.csv\")\n",
    "base_dir = \"package_images\"\n",
    "\n",
    "# Clear and recreate folder structure\n",
    "if os.path.exists(base_dir):\n",
    "    shutil.rmtree(base_dir)\n",
    "\n",
    "for label in [\"OK\", \"Damaged\"]:\n",
    "    os.makedirs(os.path.join(base_dir, label), exist_ok=True)\n",
    "\n",
    "# Generate dummy images matching filenames\n",
    "for _, row in label_df.iterrows():\n",
    "    label = row[\"ImageLabel\"]\n",
    "    name = row[\"PackageImage\"]\n",
    "    img_path = os.path.join(base_dir, label, name)\n",
    "    if not os.path.exists(img_path):\n",
    "        img = Image.fromarray(np.random.randint(0, 255, (150, 150, 3), dtype=np.uint8))\n",
    "        img.save(img_path)\n",
    "\n",
    "# STEP 3: Load Images with Corrected ImageDataGenerator\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",  # ✅ Fix: ensure binary labels\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",  # ✅ Fix here too\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# STEP 4: Define CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # ✅ For binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# STEP 5: Train & Evaluate\n",
    "model.fit(train_gen, epochs=5, validation_data=val_gen, verbose=1)\n",
    "loss, accuracy = model.evaluate(val_gen)\n",
    "print(f\"✅ Task 6 Complete — Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# STEP 6: Save Model\n",
    "model.save(\"models/task6_package_cnn.keras\")\n",
    "print(\"✅ Task 6 model saved to models/task6_package_cnn.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48a2947d-bd88-4d4c-88da-fcb68acdd3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TASK 7 — Anomaly Detection in Shipment Data with Autoencoder\n",
    "# # Author: Opemipo Oreoluwa\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Dense\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 1: Load and Preprocess Data\n",
    "# # -------------------------------\n",
    "# df = pd.read_csv(\"task7_anomaly_dataset.csv\")\n",
    "\n",
    "# # Convert dimensions to numeric volume estimate (L x W x H)\n",
    "# dims = df[\"Dimensions\"].str.split(\"x\", expand=True).astype(float)\n",
    "# df[\"DimVolume\"] = dims[0] * dims[1] * dims[2]\n",
    "\n",
    "# # Final feature set\n",
    "# X = df[[\"Weight\", \"Volume\", \"DimVolume\"]].values\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 2: Define Autoencoder\n",
    "# # -------------------------------\n",
    "# input_dim = X_scaled.shape[1]\n",
    "# encoding_dim = 2  # can be tuned\n",
    "\n",
    "# input_layer = Input(shape=(input_dim,))\n",
    "# encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "# decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "# autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 3: Train the Autoencoder\n",
    "# # -------------------------------\n",
    "# history = autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 4: Reconstruction Error and Anomaly Detection\n",
    "# # -------------------------------\n",
    "# reconstructions = autoencoder.predict(X_scaled)\n",
    "# mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)\n",
    "\n",
    "# threshold = np.percentile(mse, 95)\n",
    "# anomalies = mse > threshold\n",
    "\n",
    "# print(f\"✅ Task 7 Complete — Detected {np.sum(anomalies)} anomalies out of {len(X_scaled)} shipments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1eda2e17-0ad9-455b-8d34-bb8fef9b78ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: task7_anomaly_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# STEP 1 — Create the Task 7 Dataset from Shipment Data\n",
    "# Author: Opemipo Oreoluwa\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"shipment_logistics_dataset.csv\")\n",
    "\n",
    "# Save the necessary columns (including dimensions)\n",
    "df[[\"Weight\", \"Volume\", \"Dimensions\", \"AnomalyFlag\"]].to_csv(\"task7_anomaly_dataset.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved: task7_anomaly_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5189e4b8-6124-4cf4-bca9-6d77e146fab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "✅ Task 7 complete — Detected 500 anomalies out of 10000 samples.\n",
      "🔍 Detected vs True Anomalies:\n",
      "AnomalyFlag  DetectedAnomaly\n",
      "0            0                  9124\n",
      "             1                   481\n",
      "1            0                   376\n",
      "             1                    19\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TASK 7 — Anomaly Detection with Autoencoder\n",
    "# Author: Opemipo Oreoluwa\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1: Load and Preprocess Data\n",
    "# -------------------------------\n",
    "df = pd.read_csv(\"task7_anomaly_dataset.csv\")\n",
    "\n",
    "# Convert dimensions (e.g., \"30x20x10\") into estimated volume\n",
    "dims = df[\"Dimensions\"].str.split(\"x\", expand=True).astype(float)\n",
    "dims.columns = [\"L\", \"W\", \"H\"]\n",
    "df[\"DimVolume\"] = dims[\"L\"] * dims[\"W\"] * dims[\"H\"]\n",
    "\n",
    "# Select features for unsupervised anomaly detection\n",
    "X = df[[\"Weight\", \"Volume\", \"DimVolume\"]].values\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 2: Define Autoencoder Model\n",
    "# -------------------------------\n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim = 2\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 3: Train the Model\n",
    "# -------------------------------\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 4: Detect Anomalies\n",
    "# -------------------------------\n",
    "reconstructions = autoencoder.predict(X_scaled)\n",
    "mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)\n",
    "\n",
    "# Use 95th percentile as threshold\n",
    "threshold = np.percentile(mse, 95)\n",
    "anomalies = mse > threshold\n",
    "df[\"ReconstructionError\"] = mse\n",
    "df[\"DetectedAnomaly\"] = anomalies.astype(int)\n",
    "\n",
    "# Compare with true flags\n",
    "print(f\"✅ Task 7 complete — Detected {anomalies.sum()} anomalies out of {len(df)} samples.\")\n",
    "print(f\"🔍 Detected vs True Anomalies:\\n{df[['AnomalyFlag', 'DetectedAnomaly']].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19c750e2-ea44-474f-b0d0-95af6e378389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TASK 8 — Predictive Maintenance Classifier for Fleet Management\n",
    "# # Author: Opemipo Oreoluwa\n",
    "\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 1 — Load and Preprocess Data\n",
    "# # -------------------------------\n",
    "# # I'm using six relevant vehicle metrics to predict the likelihood of a breakdown\n",
    "# df = pd.read_csv(\"task8_maintenance_dataset.csv\")\n",
    "\n",
    "# # Split features and target\n",
    "# X = df.drop(\"BreakdownFlag\", axis=1)\n",
    "# y = df[\"BreakdownFlag\"]\n",
    "\n",
    "# # Stratified train-test split for class balance\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Normalize feature values to improve training stability\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 2 — Define a Neural Network Model\n",
    "# # -------------------------------\n",
    "# # This is a basic dense feed-forward network for binary classification\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "#     tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(1, activation=\"sigmoid\")  # sigmoid for binary output\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 3 — Train the Model\n",
    "# # -------------------------------\n",
    "# model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# # -------------------------------\n",
    "# # STEP 4 — Evaluate Performance\n",
    "# # -------------------------------\n",
    "# loss, acc = model.evaluate(X_test, y_test)\n",
    "# print(f\"✅ Task 8 Complete — Predictive Maintenance Model Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "296ec1fe-33c5-4473-8b13-dcb60f0b845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: task8_maintenance_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# TASK 8 — Extract Maintenance Dataset\n",
    "# Author: Opemipo Oreoluwa\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load your full logistics dataset\n",
    "df = pd.read_csv(\"shipment_logistics_dataset.csv\")\n",
    "\n",
    "# Select relevant columns for predictive maintenance\n",
    "df_task8 = df[[\n",
    "    \"EngineHours\", \"Mileage\", \"FuelConsumption\", \"Temperature\", \"Vibration\", \"BreakdownFlag\"\n",
    "]]\n",
    "\n",
    "# Save to CSV for the model to read\n",
    "df_task8.to_csv(\"task8_maintenance_dataset.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved: task8_maintenance_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2c1e7051-a667-4f51-a35f-eb7ef01e093e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.9248 - loss: 0.3122 - val_accuracy: 0.9675 - val_loss: 0.1464\n",
      "Epoch 2/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9724 - loss: 0.1315 - val_accuracy: 0.9675 - val_loss: 0.1464\n",
      "Epoch 3/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9714 - loss: 0.1348 - val_accuracy: 0.9675 - val_loss: 0.1451\n",
      "Epoch 4/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9701 - loss: 0.1369 - val_accuracy: 0.9675 - val_loss: 0.1453\n",
      "Epoch 5/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9703 - loss: 0.1366 - val_accuracy: 0.9675 - val_loss: 0.1451\n",
      "Epoch 6/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9720 - loss: 0.1300 - val_accuracy: 0.9675 - val_loss: 0.1428\n",
      "Epoch 7/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9733 - loss: 0.1249 - val_accuracy: 0.9675 - val_loss: 0.1439\n",
      "Epoch 8/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9673 - loss: 0.1448 - val_accuracy: 0.9675 - val_loss: 0.1448\n",
      "Epoch 9/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9705 - loss: 0.1346 - val_accuracy: 0.9675 - val_loss: 0.1442\n",
      "Epoch 10/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.1249 - val_accuracy: 0.9675 - val_loss: 0.1422\n",
      "Epoch 11/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9692 - loss: 0.1367 - val_accuracy: 0.9675 - val_loss: 0.1456\n",
      "Epoch 12/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9692 - loss: 0.1387 - val_accuracy: 0.9675 - val_loss: 0.1446\n",
      "Epoch 13/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9708 - loss: 0.1305 - val_accuracy: 0.9675 - val_loss: 0.1429\n",
      "Epoch 14/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9709 - loss: 0.1276 - val_accuracy: 0.9675 - val_loss: 0.1431\n",
      "Epoch 15/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9739 - loss: 0.1191 - val_accuracy: 0.9675 - val_loss: 0.1445\n",
      "Epoch 16/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9718 - loss: 0.1239 - val_accuracy: 0.9675 - val_loss: 0.1426\n",
      "Epoch 17/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1222 - val_accuracy: 0.9675 - val_loss: 0.1429\n",
      "Epoch 18/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9745 - loss: 0.1157 - val_accuracy: 0.9675 - val_loss: 0.1465\n",
      "Epoch 19/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9679 - loss: 0.1377 - val_accuracy: 0.9675 - val_loss: 0.1452\n",
      "Epoch 20/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9710 - loss: 0.1286 - val_accuracy: 0.9675 - val_loss: 0.1459\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9656 - loss: 0.1532\n",
      "✅ Task 8 Complete — Predictive Maintenance Model Accuracy: 0.9705\n"
     ]
    }
   ],
   "source": [
    "# TASK 8 — Predictive Maintenance Classifier for Fleet Management\n",
    "# Author: Opemipo Oreoluwa\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and preprocess dataset\n",
    "df = pd.read_csv(\"task8_maintenance_dataset.csv\")\n",
    "\n",
    "X = df.drop(\"BreakdownFlag\", axis=1)\n",
    "y = df[\"BreakdownFlag\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate performance\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"✅ Task 8 Complete — Predictive Maintenance Model Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9992d7b-9110-4172-97db-98b327ad1756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # streamlit_app/app.py\n",
    "# import streamlit as st\n",
    "# import pandas as pd\n",
    "# import joblib\n",
    "# import numpy as np\n",
    "\n",
    "# # Load trained models\n",
    "# delivery_model = joblib.load(\"models/delivery_model.pkl\")\n",
    "# breakdown_model = joblib.load(\"models/breakdown_model.pkl\")\n",
    "\n",
    "# st.set_page_config(page_title=\"HeavyHaul AI\", layout=\"centered\")\n",
    "\n",
    "# st.title(\"🚚 HeavyHaul Systems — Smart Logistics Assistant\")\n",
    "# st.markdown(\"Apply ML predictions to your shipment and fleet data.\")\n",
    "\n",
    "# # ---- Sidebar navigation ----\n",
    "# task = st.sidebar.selectbox(\"Select Task\", [\n",
    "#     \"Predict Shipment Delivery Time\",\n",
    "#     \"Predict Breakdown Risk\",\n",
    "#     \"Show Shipment Anomalies\"\n",
    "# ])\n",
    "\n",
    "# # ---- Task 1: Predict Delivery Time ----\n",
    "# if task == \"Predict Shipment Delivery Time\":\n",
    "#     st.subheader(\"📦 Predict Delivery Time\")\n",
    "#     with st.form(\"delivery_form\"):\n",
    "#         origin = st.selectbox(\"Origin\", [\"New York\", \"Chicago\", \"Miami\", \"Boston\", \"Dallas\"])\n",
    "#         destination = st.selectbox(\"Destination\", [\"Los Angeles\", \"Houston\", \"Seattle\", \"Denver\"])\n",
    "#         carrier = st.selectbox(\"Carrier\", [\"FedEx\", \"UPS\", \"DHL\"])\n",
    "#         shipment_type = st.selectbox(\"Shipment Type\", [\"Standard\", \"Express\", \"Fragile\"])\n",
    "#         weight = st.number_input(\"Weight (kg)\", min_value=10.0, max_value=500.0)\n",
    "#         volume = st.number_input(\"Volume (m³)\", min_value=0.5, max_value=5.0)\n",
    "#         dims = st.text_input(\"Dimensions (LxWxH)\", value=\"30x20x10\")\n",
    "\n",
    "#         submitted = st.form_submit_button(\"Predict Delivery Time\")\n",
    "#         if submitted:\n",
    "#             dims_split = [int(x) for x in dims.lower().split(\"x\")]\n",
    "#             if len(dims_split) != 3:\n",
    "#                 st.error(\"❌ Please use format: 30x20x10\")\n",
    "#             else:\n",
    "#                 # Create dataframe for model\n",
    "#                 input_df = pd.DataFrame({\n",
    "#                     \"Origin\": [origin],\n",
    "#                     \"Destination\": [destination],\n",
    "#                     \"Carrier\": [carrier],\n",
    "#                     \"ShipmentType\": [shipment_type],\n",
    "#                     \"Weight\": [weight],\n",
    "#                     \"Volume\": [volume],\n",
    "#                     \"Dimensions\": [f\"{dims_split[0]}x{dims_split[1]}x{dims_split[2]}\"]\n",
    "#                 })\n",
    "#                 pred_days = delivery_model.predict(input_df)[0]\n",
    "#                 st.success(f\"✅ Estimated Delivery Time: **{round(pred_days, 2)} days**\")\n",
    "\n",
    "# # ---- Task 8: Predict Breakdown ----\n",
    "# elif task == \"Predict Breakdown Risk\":\n",
    "#     st.subheader(\"🔧 Predict Breakdown Risk\")\n",
    "#     with st.form(\"breakdown_form\"):\n",
    "#         engine_hours = st.slider(\"Engine Hours\", 1000, 8000, 5000)\n",
    "#         mileage = st.slider(\"Mileage (km)\", 20000, 150000, 50000)\n",
    "#         fuel = st.slider(\"Fuel Consumption (L/100km)\", 4.0, 15.0, 8.5)\n",
    "#         temp = st.slider(\"Engine Temperature (°C)\", 10, 90, 45)\n",
    "#         vibration = st.slider(\"Vibration Level\", 0.01, 0.10, 0.05)\n",
    "\n",
    "#         submit2 = st.form_submit_button(\"Predict Breakdown Risk\")\n",
    "#         if submit2:\n",
    "#             X_input = pd.DataFrame([[\n",
    "#                 engine_hours, mileage, fuel, temp, vibration\n",
    "#             ]], columns=[\"EngineHours\", \"Mileage\", \"FuelConsumption\", \"Temperature\", \"Vibration\"])\n",
    "#             pred = breakdown_model.predict(X_input)[0]\n",
    "#             prob = breakdown_model.predict_proba(X_input)[0][1]\n",
    "#             if pred == 1:\n",
    "#                 st.error(f\"⚠️ High Breakdown Risk (Confidence: {prob:.2%})\")\n",
    "#             else:\n",
    "#                 st.success(f\"✅ No Immediate Breakdown Risk (Confidence: {prob:.2%})\")\n",
    "\n",
    "# # ---- Task 7: Show Anomalies ----\n",
    "# elif task == \"Show Shipment Anomalies\":\n",
    "#     st.subheader(\"🚨 Shipment Anomaly Insights\")\n",
    "#     df = pd.read_csv(\"data/anomaly_detection.csv\")\n",
    "#     st.dataframe(df[df[\"AnomalyFlag\"] == 1].head(10))\n",
    "#     st.markdown(\"Shows first 10 detected anomalies in the shipment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f82fa84d-b856-41a7-b1f6-23df5a2770fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_delivery_model.py\n",
    "# import pandas as pd\n",
    "# import joblib\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # Load your cleaned dataset\n",
    "# df = pd.read_csv(\"shipment_times_for_ml.csv\", parse_dates=[\"ActualShipDate\", \"ActualDeliveryDate\"])\n",
    "\n",
    "# # Target variable\n",
    "# df[\"DeliveryTimeDays\"] = (df[\"ActualDeliveryDate\"] - df[\"ActualShipDate\"]).dt.days\n",
    "\n",
    "# # Drop any rows with missing target\n",
    "# df.dropna(subset=[\"DeliveryTimeDays\"], inplace=True)\n",
    "\n",
    "# # Select features and target\n",
    "# X = df[[\"Origin\", \"Destination\", \"Carrier\", \"ShipmentType\", \"Weight\", \"Volume\", \"Dimensions\"]]\n",
    "# y = df[\"DeliveryTimeDays\"]\n",
    "\n",
    "# # Preprocessing\n",
    "# categorical = [\"Origin\", \"Destination\", \"Carrier\", \"ShipmentType\", \"Dimensions\"]\n",
    "# numerical = [\"Weight\", \"Volume\"]\n",
    "\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical),\n",
    "#     (\"num\", StandardScaler(), numerical)\n",
    "# ])\n",
    "\n",
    "# # Pipeline\n",
    "# model = Pipeline([\n",
    "#     (\"pre\", preprocessor),\n",
    "#     (\"reg\", RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "# ])\n",
    "\n",
    "# # Train and save model\n",
    "# model.fit(X, y)\n",
    "# joblib.dump(model, \"models/delivery_model.pkl\")\n",
    "# print(\"✅ delivery_model.pkl saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "014cb72a-cb7a-4b44-80f3-1a4bf66d955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create task1-3 dataset from the main shipment_logistics_dataset\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load full dataset\n",
    "# df = pd.read_csv(\"shipment_logistics_dataset.csv\", parse_dates=[\"ActualShipDate\", \"ActualDeliveryDate\"])\n",
    "\n",
    "# # Compute delivery time in days\n",
    "# df[\"DeliveryTimeDays\"] = (df[\"ActualDeliveryDate\"] - df[\"ActualShipDate\"]).dt.days\n",
    "\n",
    "# # Select relevant features\n",
    "# shipment_df = df[[\n",
    "#     \"Origin\", \"Destination\", \"Carrier\", \"ShipmentType\",\n",
    "#     \"Weight\", \"Volume\", \"Dimensions\",\n",
    "#     \"PlannedShipDate\", \"ActualShipDate\",\n",
    "#     \"PlannedDeliveryDate\", \"ActualDeliveryDate\",\n",
    "#     \"Status\", \"DeliveryTimeDays\"\n",
    "# ]].dropna()\n",
    "\n",
    "# shipment_df.to_csv(\"shipment_times_for_ml.csv\", index=False)\n",
    "# print(\"✅ Saved: shipment_times_for_ml.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f0f9bd6-19d3-4be0-9d48-89759addb094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_delivery_model.py\n",
    "# import pandas as pd\n",
    "# import joblib\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # Load the new shipment_times_for_ml.csv\n",
    "# df = pd.read_csv(\"shipment_times_for_ml.csv\")\n",
    "\n",
    "# # Target\n",
    "# X = df[[\"Origin\", \"Destination\", \"Carrier\", \"ShipmentType\", \"Weight\", \"Volume\", \"Dimensions\"]]\n",
    "# y = df[\"DeliveryTimeDays\"]\n",
    "\n",
    "# # Preprocessing pipeline\n",
    "# cat = [\"Origin\", \"Destination\", \"Carrier\", \"ShipmentType\", \"Dimensions\"]\n",
    "# num = [\"Weight\", \"Volume\"]\n",
    "\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat),\n",
    "#     (\"num\", StandardScaler(), num)\n",
    "# ])\n",
    "\n",
    "# # Pipeline\n",
    "# model = Pipeline([\n",
    "#     (\"prep\", preprocessor),\n",
    "#     (\"reg\", RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "# ])\n",
    "\n",
    "# model.fit(X, y)\n",
    "# joblib.dump(model, \"models/delivery_model.pkl\")\n",
    "# print(\"✅ delivery_model.pkl saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2afb3d66-53ec-4149-984c-48209775d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ✅ This script should be run in Jupyter or VS Code to train and save the models used in your Streamlit app\n",
    "# # Assumes `shipment_logistics_dataset.csv` is in the same directory\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import joblib\n",
    "# import torch\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # Create models directory\n",
    "# os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# # Load base dataset\n",
    "# df = pd.read_csv(\"shipment_logistics_dataset.csv\", parse_dates=[\"ActualShipDate\", \"ActualDeliveryDate\"])\n",
    "\n",
    "# # ----------------------------\n",
    "# # Task 1–3: Delivery Time Model\n",
    "# # ----------------------------\n",
    "# df[\"DeliveryTimeDays\"] = (df[\"ActualDeliveryDate\"] - df[\"ActualShipDate\"]).dt.days\n",
    "\n",
    "# df1 = df[[\"Origin\", \"Destination\", \"Carrier\", \"ShipmentType\", \"Weight\", \"Volume\", \"Dimensions\", \"DeliveryTimeDays\"]].dropna()\n",
    "\n",
    "# X = df1.drop(\"DeliveryTimeDays\", axis=1)\n",
    "# y = df1[\"DeliveryTimeDays\"]\n",
    "\n",
    "# categorical = [\"Origin\", \"Destination\", \"Carrier\", \"ShipmentType\", \"Dimensions\"]\n",
    "# numerical = [\"Weight\", \"Volume\"]\n",
    "\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical),\n",
    "#     (\"num\", StandardScaler(), numerical)\n",
    "# ])\n",
    "\n",
    "# model = Pipeline([\n",
    "#     (\"prep\", preprocessor),\n",
    "#     (\"reg\", RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "# ])\n",
    "\n",
    "# model.fit(X, y)\n",
    "# joblib.dump(model, \"models/delivery_model.pkl\")\n",
    "# print(\"✅ delivery_model.pkl saved\")\n",
    "\n",
    "# # ----------------------------\n",
    "# # Task 4: Disruption Classifier (PyTorch)\n",
    "# # ----------------------------\n",
    "# df[\"DisruptionFlag\"] = (df[\"Status\"] == \"Disrupted\").astype(int)\n",
    "# df2 = df[[\"Weight\", \"Volume\", \"Weather\", \"Traffic\", \"PublicEvent\", \"DisruptionFlag\"]].dropna()\n",
    "\n",
    "# X = df2.drop(\"DisruptionFlag\", axis=1)\n",
    "# y = df2[\"DisruptionFlag\"]\n",
    "\n",
    "# for col in [\"Weather\", \"Traffic\", \"PublicEvent\"]:\n",
    "#     X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "# X = torch.tensor(X.values, dtype=torch.float32)\n",
    "# y = torch.tensor(y.values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# class DisruptionModel(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.net = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(X.shape[1], 16),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(16, 1),\n",
    "#             torch.nn.Sigmoid()\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# model_d = DisruptionModel()\n",
    "# loss_fn = torch.nn.BCELoss()\n",
    "# optimizer = torch.optim.Adam(model_d.parameters(), lr=0.01)\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model_d(X)\n",
    "#     loss = loss_fn(output, y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# torch.save(model_d.state_dict(), \"models/disruption_model.pt\")\n",
    "# print(\"✅ disruption_model.pt saved\")\n",
    "\n",
    "# # ----------------------------\n",
    "# # Task 5: Forecasting Model\n",
    "# # ----------------------------\n",
    "# timeseries_df = df.groupby(\"ActualShipDate\")[\"DeliveryTimeDays\"].mean().reset_index()\n",
    "# timeseries_df[\"AvgDeliveryTime\"] = timeseries_df[\"DeliveryTimeDays\"]\n",
    "# timeseries_df[\"day_index\"] = np.arange(len(timeseries_df))\n",
    "\n",
    "# X = timeseries_df[[\"day_index\"]]\n",
    "# y = timeseries_df[[\"AvgDeliveryTime\"]]\n",
    "\n",
    "# model_f = tf.keras.Sequential([\n",
    "#     layers.Dense(32, activation=\"relu\", input_shape=(1,)),\n",
    "#     layers.Dense(16, activation=\"relu\"),\n",
    "#     layers.Dense(1)\n",
    "# ])\n",
    "# model_f.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "# model_f.fit(X, y, epochs=30, verbose=0)\n",
    "# model_f.save(\"models/forecasting_model.keras\")\n",
    "# print(\"✅ forecasting_model.keras saved\")\n",
    "\n",
    "# # ----------------------------\n",
    "# # Task 6: CNN Image Model (mock only — assume real retraining outside)\n",
    "# # ----------------------------\n",
    "# # You can place your trained model here\n",
    "# # For now, we create and save a dummy CNN model\n",
    "# image_model = tf.keras.Sequential([\n",
    "#     layers.Input(shape=(150,150,3)),\n",
    "#     layers.Conv2D(16, (3,3), activation='relu'),\n",
    "#     layers.MaxPooling2D(2,2),\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     layers.Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "# image_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "# image_model.save(\"models/cnn_package_model.keras\")\n",
    "# print(\"✅ cnn_package_model.keras saved (dummy)\")\n",
    "\n",
    "# # ----------------------------\n",
    "# # Task 7: Anomaly Autoencoder\n",
    "# # ----------------------------\n",
    "# df7 = df[[\"Weight\", \"Volume\"]].dropna()\n",
    "# X = StandardScaler().fit_transform(df7.values)\n",
    "\n",
    "# inp = layers.Input(shape=(X.shape[1],))\n",
    "# encoded = layers.Dense(2, activation='relu')(inp)\n",
    "# decoded = layers.Dense(X.shape[1], activation='sigmoid')(encoded)\n",
    "# autoencoder = tf.keras.Model(inp, decoded)\n",
    "# autoencoder.compile(optimizer='adam', loss='mse')\n",
    "# autoencoder.fit(X, X, epochs=20, verbose=0)\n",
    "# autoencoder.save(\"models/anomaly_autoencoder.keras\")\n",
    "# print(\"✅ anomaly_autoencoder.keras saved\")\n",
    "\n",
    "# # ----------------------------\n",
    "# # Task 8: Predictive Maintenance\n",
    "# # ----------------------------\n",
    "# df8 = df[[\"EngineHours\", \"Mileage\", \"FuelConsumption\", \"Temperature\", \"Vibration\", \"BreakdownFlag\"]].dropna()\n",
    "# X = df8.drop(\"BreakdownFlag\", axis=1)\n",
    "# y = df8[\"BreakdownFlag\"]\n",
    "\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# model_m = tf.keras.Sequential([\n",
    "#     layers.Dense(32, activation=\"relu\", input_shape=(X.shape[1],)),\n",
    "#     layers.Dense(16, activation=\"relu\"),\n",
    "#     layers.Dense(1, activation=\"sigmoid\")\n",
    "# ])\n",
    "# model_m.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "# model_m.fit(X, y, epochs=20, verbose=0)\n",
    "# model_m.save(\"models/maintenance_model.keras\")\n",
    "# print(\"✅ maintenance_model.keras saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "135a7fbf-e2b2-4ee1-a15b-07b7ff5048ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "\n",
    "# # 📁 Create directory for saved models if it doesn't exist\n",
    "# os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# # ✅ Task 3 — TensorFlow Neural Net for Delivery Time Prediction\n",
    "# try:\n",
    "#     model.save(\"models/task3_delivery_time_nn.h5\")\n",
    "#     print(\"✅ Task 3 model saved: task3_delivery_time_nn.h5\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Task 3 save error: {e}\")\n",
    "\n",
    "# # ✅ Task 4 — PyTorch Disruption Classifier\n",
    "# try:\n",
    "#     torch.save(model.state_dict(), \"models/task4_disruption_model.pt\")\n",
    "#     print(\"✅ Task 4 model saved: task4_disruption_model.pt\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Task 4 save error: {e}\")\n",
    "\n",
    "# # ✅ Task 5 — LSTM Time Series Forecasting Model\n",
    "# try:\n",
    "#     model.save(\"models/task5_timeseries_lstm.h5\")\n",
    "#     print(\"✅ Task 5 model saved: task5_timeseries_lstm.h5\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Task 5 save error: {e}\")\n",
    "\n",
    "# # ✅ Task 6 — CNN Image Classification Model\n",
    "# try:\n",
    "#     model.save(\"models/task6_package_cnn.h5\")\n",
    "#     print(\"✅ Task 6 model saved: task6_package_cnn.h5\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Task 6 save error: {e}\")\n",
    "\n",
    "# # ✅ Task 7 — Autoencoder for Anomaly Detection\n",
    "# try:\n",
    "#     autoencoder.save(\"models/task7_anomaly_autoencoder.h5\")\n",
    "#     print(\"✅ Task 7 model saved: task7_anomaly_autoencoder.h5\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Task 7 save error: {e}\")\n",
    "\n",
    "# # ✅ Task 8 — Maintenance Classifier\n",
    "# try:\n",
    "#     model.save(\"models/task8_maintenance_classifier.h5\")\n",
    "#     print(\"✅ Task 8 model saved: task8_maintenance_classifier.h5\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Task 8 save error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "173887ff-6030-4ae7-82cd-3e5e3fed3200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import joblib\n",
    "# from tensorflow.keras.models import save_model\n",
    "\n",
    "# os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# # TASK 3 — Neural Net for Delivery Time (Keras model)\n",
    "# try:\n",
    "#     save_model(task3_model, \"models/task3_delivery_time_nn.keras\")\n",
    "#     print(\"✅ Task 3 model saved: task3_delivery_time_nn.keras\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Task 3 save error: {e}\")\n",
    "\n",
    "# # TASK 4 — PyTorch model for Disruption\n",
    "# try:\n",
    "#     torch.save(task4_model.state_dict(), \"models/task4_disruption_model.pt\")\n",
    "#     print(\"✅ Task 4 model saved: task4_disruption_model.pt\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Task 4 save error: {e}\")\n",
    "\n",
    "# # TASK 5 — LSTM Time Series (Keras)\n",
    "# try:\n",
    "#     save_model(task5_model, \"models/task5_timeseries_lstm.keras\")\n",
    "#     print(\"✅ Task 5 model saved: task5_timeseries_lstm.keras\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Task 5 save error: {e}\")\n",
    "\n",
    "# # TASK 6 — CNN for Images (Keras)\n",
    "# try:\n",
    "#     save_model(task6_model, \"models/task6_package_cnn.keras\")\n",
    "#     print(\"✅ Task 6 model saved: task6_package_cnn.keras\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Task 6 save error: {e}\")\n",
    "\n",
    "# # TASK 7 — Autoencoder (Keras)\n",
    "# try:\n",
    "#     autoencoder.save(\"models/task7_anomaly_autoencoder.keras\")\n",
    "#     print(\"✅ Task 7 model saved: task7_anomaly_autoencoder.keras\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Task 7 save error: {e}\")\n",
    "\n",
    "# # TASK 8 — Maintenance Classifier (Keras)\n",
    "# try:\n",
    "#     save_model(task8_model, \"models/task8_maintenance_classifier.keras\")\n",
    "#     print(\"✅ Task 8 model saved: task8_maintenance_classifier.keras\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Task 8 save error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "feab4a8f-e46b-4d6e-923d-9d8b34edbd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you've trained them in sequence using variable name 'model', reassign here:\n",
    "# task3_model = model  # after Task 3 training\n",
    "# task4_model = model  # after Task 4 training (PyTorch)\n",
    "# task5_model = model  # after Task 5 training (LSTM)\n",
    "# task6_model = model  # after Task 6 training (CNN)\n",
    "# # Task 7 model was saved correctly\n",
    "# task8_model = model  # after Task 8 training (Keras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a870f4c-a06f-42fd-b454-7b34d19cf30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task4_model = DisruptionPredictor()\n",
    "# task4_model.load_state_dict(torch.load(\"models/task4_disruption_model.pt\"))  # if saved earlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da7b6e1a-5151-4431-9b63-6e7183f988c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from tensorflow.keras.models import save_model\n",
    "\n",
    "# os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# try:\n",
    "#     save_model(task3_model, \"models/task3_delivery_time_nn.keras\")\n",
    "#     print(\"✅ Task 3 model saved\")\n",
    "# except Exception as e:\n",
    "#     print(\"❌ Task 3:\", e)\n",
    "\n",
    "# try:\n",
    "#     torch.save(task4_model.state_dict(), \"models/task4_disruption_model.pt\")\n",
    "#     print(\"✅ Task 4 model saved\")\n",
    "# except Exception as e:\n",
    "#     print(\"❌ Task 4:\", e)\n",
    "\n",
    "# try:\n",
    "#     save_model(task5_model, \"models/task5_timeseries_lstm.keras\")\n",
    "#     print(\"✅ Task 5 model saved\")\n",
    "# except Exception as e:\n",
    "#     print(\"❌ Task 5:\", e)\n",
    "\n",
    "# try:\n",
    "#     save_model(task6_model, \"models/task6_package_cnn.keras\")\n",
    "#     print(\"✅ Task 6 model saved\")\n",
    "# except Exception as e:\n",
    "#     print(\"❌ Task 6:\", e)\n",
    "\n",
    "# try:\n",
    "#     autoencoder.save(\"models/task7_anomaly_autoencoder.keras\")\n",
    "#     print(\"✅ Task 7 model saved\")\n",
    "# except Exception as e:\n",
    "#     print(\"❌ Task 7:\", e)\n",
    "\n",
    "# try:\n",
    "#     save_model(task8_model, \"models/task8_maintenance_classifier.keras\")\n",
    "#     print(\"✅ Task 8 model saved\")\n",
    "# except Exception as e:\n",
    "#     print(\"❌ Task 8:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6fe818be-7a9d-4b45-b003-3ccad08ed937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task 4 model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Task 4 — Disruption Prediction (retrain and save)\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"shipment_logistics_dataset.csv\", parse_dates=[\"ActualShipDate\"])\n",
    "df[\"ActualShipDate\"] = df[\"ActualShipDate\"].dt.date\n",
    "df[\"Route\"] = df[\"RouteID\"]\n",
    "df[\"DisruptionFlag\"] = (df[\"Status\"] == \"Disrupted\").astype(int)\n",
    "\n",
    "# Create weather data and merge\n",
    "weather_conditions = [\"Clear\", \"Rain\", \"Storm\", \"Cloudy\", \"Sunny\"]\n",
    "weather_data = df[[\"ActualShipDate\", \"Route\"]].drop_duplicates().copy()\n",
    "weather_data[\"weather\"] = [random.choice(weather_conditions) for _ in range(len(weather_data))]\n",
    "\n",
    "merged_df = pd.merge(df, weather_data, left_on=[\"ActualShipDate\", \"Route\"], right_on=[\"ActualShipDate\", \"Route\"])\n",
    "merged_df[\"weather_encoded\"] = LabelEncoder().fit_transform(merged_df[\"weather\"])\n",
    "merged_df[\"route_encoded\"] = LabelEncoder().fit_transform(merged_df[\"Route\"])\n",
    "\n",
    "# Prepare tensors\n",
    "X = merged_df[[\"route_encoded\", \"Volume\", \"weather_encoded\"]].values\n",
    "y = merged_df[\"DisruptionFlag\"].values.astype(float)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Model\n",
    "class DisruptionPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x))\n",
    "\n",
    "task4_model = DisruptionPredictor()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(task4_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train\n",
    "for epoch in range(10):\n",
    "    task4_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = task4_model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Save\n",
    "torch.save(task4_model.state_dict(), \"models/task4_disruption_model.pt\")\n",
    "print(\"✅ Task 4 model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "11679848-87c1-4a28-a32b-e7ab5f81a3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task 3 model saved\n",
      "❌ Task 4: 'Sequential' object has no attribute 'state_dict'\n",
      "✅ Task 5 model saved\n",
      "✅ Task 6 model saved\n",
      "✅ Task 7 model saved\n",
      "✅ Task 8 model saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# Create models/ directory if not exists\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Task 3 — Neural Network (TensorFlow)\n",
    "try:\n",
    "    save_model(model, \"models/task3_delivery_nn.keras\")  # model is the trained Keras model\n",
    "    print(\"✅ Task 3 model saved\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Task 3:\", e)\n",
    "\n",
    "# Task 4 — PyTorch Disruption Classifier\n",
    "try:\n",
    "    torch.save(model.state_dict(), \"models/task4_disruption_model.pt\")  # model is the PyTorch DisruptionPredictor\n",
    "    print(\"✅ Task 4 model saved\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Task 4:\", e)\n",
    "\n",
    "# Task 5 — LSTM for Time Series (TensorFlow)\n",
    "try:\n",
    "    save_model(model, \"models/task5_timeseries_lstm.keras\")\n",
    "    print(\"✅ Task 5 model saved\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Task 5:\", e)\n",
    "\n",
    "# Task 6 — CNN for Image Classification (TensorFlow)\n",
    "try:\n",
    "    save_model(model, \"models/task6_package_cnn.keras\")\n",
    "    print(\"✅ Task 6 model saved\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Task 6:\", e)\n",
    "\n",
    "# Task 7 — Autoencoder for Anomaly Detection\n",
    "try:\n",
    "    autoencoder.save(\"models/task7_anomaly_autoencoder.keras\")\n",
    "    print(\"✅ Task 7 model saved\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Task 7:\", e)\n",
    "\n",
    "# Task 8 — Predictive Maintenance Classifier (TensorFlow)\n",
    "try:\n",
    "    save_model(model, \"models/task8_maintenance_classifier.keras\")\n",
    "    print(\"✅ Task 8 model saved\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Task 8:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f1b5da2f-8aa9-443d-9f9a-6bb94126090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "98d1964d-c378-40aa-a436-cbfda3c17623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Task 3 model\n",
    "# task3_model = load_model(\"models/task3_delivery_nn.h5\")  # or .keras if you haven’t re-saved yet\n",
    "\n",
    "# # Sample input: Weight, Volume\n",
    "# task3_input = np.array([[150, 3.5]])\n",
    "# pred = task3_model.predict(task3_input)\n",
    "# print(f\"📦 Task 3: Predicted Delivery Time = {pred[0][0]:.2f} days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a8110014-c539-490c-b27a-5874da3738a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anomaly_autoencoder.keras', 'cnn_package_model.keras', 'delivery_model.pkl', 'disruption_model.pt', 'forecasting_model.keras', 'maintenance_model.keras', 'task3_delivery_nn.keras', 'task4_disruption_model.pt', 'task5_timeseries_lstm.keras', 'task6_package_cnn.keras', 'task7_anomaly_autoencoder.h5', 'task7_anomaly_autoencoder.keras', 'task8_maintenance_classifier.keras']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"models\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1eddedbd-db9e-4800-89c6-65802060680b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['anomaly_autoencoder.keras', 'cnn_package_model.keras', 'delivery_model.pkl', 'disruption_model.pt', 'forecasting_model.keras', 'maintenance_model.keras', 'task3_delivery_nn.keras', 'task4_disruption_model.pt', 'task5_timeseries_lstm.keras', 'task6_package_cnn.keras', 'task7_anomaly_autoencoder.h5', 'task7_anomaly_autoencoder.keras', 'task8_maintenance_classifier.keras']\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_model_def'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m task3_model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/task3_delivery_nn.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Task 4: Disruption Classifier (PyTorch)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_model_def\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DisruptionPredictor  \u001b[38;5;66;03m# this is your defined class\u001b[39;00m\n\u001b[0;32m     14\u001b[0m task4_model \u001b[38;5;241m=\u001b[39m DisruptionPredictor()\n\u001b[0;32m     15\u001b[0m task4_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/task4_disruption_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch_model_def'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "\n",
    "# Verify models exist\n",
    "print(\"Available models:\", os.listdir(\"models\"))\n",
    "\n",
    "# Task 3: Delivery Neural Network (TensorFlow)\n",
    "task3_model = load_model(\"models/task3_delivery_nn.keras\")\n",
    "\n",
    "# Task 4: Disruption Classifier (PyTorch)\n",
    "from torch_model_def import DisruptionPredictor  # this is your defined class\n",
    "task4_model = DisruptionPredictor()\n",
    "task4_model.load_state_dict(torch.load(\"models/task4_disruption_model.pt\"))\n",
    "task4_model.eval()\n",
    "\n",
    "# Task 5: LSTM Forecasting Model (TensorFlow)\n",
    "task5_model = load_model(\"models/task5_timeseries_lstm.keras\")\n",
    "\n",
    "# Task 6: CNN for Image Classification (TensorFlow)\n",
    "task6_model = load_model(\"models/task6_package_cnn.keras\")\n",
    "\n",
    "# Task 7: Autoencoder for Anomaly Detection (TensorFlow)\n",
    "task7_model = load_model(\"models/task7_anomaly_autoencoder.keras\")\n",
    "\n",
    "# Task 8: Predictive Maintenance Model (TensorFlow)\n",
    "task8_model = load_model(\"models/task8_maintenance_classifier.keras\")\n",
    "\n",
    "print(\"✅ All models loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c28a4d19-fdf3-431d-8335-134441ea0efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Available models: ['anomaly_autoencoder.keras', 'cnn_package_model.keras', 'delivery_model.pkl', 'disruption_model.pt', 'forecasting_model.keras', 'maintenance_model.keras', 'task3_delivery_nn.keras', 'task4_disruption_model.pt', 'task5_timeseries_lstm.keras', 'task6_package_cnn.keras', 'task7_anomaly_autoencoder.h5', 'task7_anomaly_autoencoder.keras', 'task8_maintenance_classifier.keras']\n",
      "✅ All models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Check saved models\n",
    "print(\"✅ Available models:\", os.listdir(\"models\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Task 3 — TensorFlow Delivery NN\n",
    "# -----------------------------\n",
    "task3_model = load_model(\"models/task3_delivery_nn.keras\")\n",
    "\n",
    "# -----------------------------\n",
    "# Task 4 — PyTorch Disruption Classifier\n",
    "# -----------------------------\n",
    "class DisruptionPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x))\n",
    "\n",
    "task4_model = DisruptionPredictor()\n",
    "task4_model.load_state_dict(torch.load(\"models/task4_disruption_model.pt\"))\n",
    "task4_model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# Task 5 — Time Series LSTM\n",
    "# -----------------------------\n",
    "task5_model = load_model(\"models/task5_timeseries_lstm.keras\")\n",
    "\n",
    "# -----------------------------\n",
    "# Task 6 — CNN for Image Classification\n",
    "# -----------------------------\n",
    "task6_model = load_model(\"models/task6_package_cnn.keras\")\n",
    "\n",
    "# -----------------------------\n",
    "# Task 7 — Autoencoder\n",
    "# -----------------------------\n",
    "task7_model = load_model(\"models/task7_anomaly_autoencoder.keras\")\n",
    "\n",
    "# -----------------------------\n",
    "# Task 8 — Predictive Maintenance Classifier\n",
    "# -----------------------------\n",
    "task8_model = load_model(\"models/task8_maintenance_classifier.keras\")\n",
    "\n",
    "print(\"✅ All models loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5415f894-3091-42ef-b9df-58be2fb3367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# Save model using Keras zip format (recommended)\n",
    "model.save(\"models/task3_delivery_nn.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2eec819d-d82e-42f9-8cb4-a6420c69d9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task 3 model saved\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "import os\n",
    "\n",
    "# Create models/ directory if not exists\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Assuming `model` is your trained Task 3 model\n",
    "save_model(model, \"models/task3_delivery_nn.keras\")\n",
    "print(\"✅ Task 3 model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "564b98ee-3350-4c9e-830c-ae3dd4067a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task 3 model saved to heavyhaul/models/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# Correct path for Streamlit to find the file\n",
    "os.makedirs(\"C:/Users/USER PC/Downloads/heavyhaul/models\", exist_ok=True)\n",
    "save_model(model, \"C:/Users/USER PC/Downloads/heavyhaul/models/task3_delivery_nn.keras\")\n",
    "print(\"✅ Task 3 model saved to heavyhaul/models/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e1f3931a-e3f7-4ae0-ae7d-8e06cc46abbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Saving models to: C:/Users/USER PC/Downloads/heavyhaul/models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task 3 saved\n",
      "✅ Task 4 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task 5 saved\n",
      "Found 8001 images belonging to 2 classes.\n",
      "Found 1999 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\USER PC\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Task 6 error: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 2), output.shape=(None, 1)\n",
      "✅ Task 7 saved\n",
      "✅ Task 8 saved\n"
     ]
    }
   ],
   "source": [
    "# save_all_models.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# Create consistent model save path\n",
    "model_dir = r\"C:/Users/USER PC/Downloads/heavyhaul/models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(\"📦 Saving models to:\", model_dir)\n",
    "\n",
    "# --------------------------\n",
    "# TASK 3 — Delivery NN (TF)\n",
    "# --------------------------\n",
    "try:\n",
    "    df = pd.read_csv(\"shipment_logistics_dataset.csv\")\n",
    "    df[\"DeliveryTimeDays\"] = (\n",
    "        pd.to_datetime(df[\"ActualDeliveryDate\"]) -\n",
    "        pd.to_datetime(df[\"ActualShipDate\"])\n",
    "    ).dt.days\n",
    "\n",
    "    X = df[[\"Weight\", \"Volume\"]].dropna()\n",
    "    y = df.loc[X.index, \"DeliveryTimeDays\"]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, _, y_train, _ = train_test_split(X_scaled, y, test_size=0.2)\n",
    "    task3_model = tf.keras.Sequential([\n",
    "        Dense(32, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    task3_model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    task3_model.fit(X_train, y_train, epochs=5, verbose=0)\n",
    "\n",
    "    save_model(task3_model, os.path.join(model_dir, \"task3_delivery_nn.keras\"))\n",
    "    print(\"✅ Task 3 saved\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Task 3 error:\", e)\n",
    "\n",
    "# --------------------------\n",
    "# TASK 4 — PyTorch Classifier\n",
    "# --------------------------\n",
    "try:\n",
    "    class DisruptionPredictor(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(3, 64)\n",
    "            self.fc2 = nn.Linear(64, 32)\n",
    "            self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            return torch.sigmoid(self.fc3(x))\n",
    "\n",
    "    df = pd.read_csv(\"shipment_logistics_dataset.csv\", parse_dates=[\"ActualShipDate\"])\n",
    "    df[\"ActualShipDate\"] = df[\"ActualShipDate\"].dt.date\n",
    "    df[\"Route\"] = df[\"RouteID\"]\n",
    "    df[\"DisruptionFlag\"] = (df[\"Status\"] == \"Disrupted\").astype(int)\n",
    "\n",
    "    # Match dates and routes\n",
    "    weather = pd.read_csv(\"weather_data.csv\")\n",
    "    merged = pd.merge(df, weather, left_on=[\"ActualShipDate\", \"Route\"], right_on=[\"date\", \"route\"])\n",
    "    merged[\"weather_encoded\"] = LabelEncoder().fit_transform(merged[\"weather\"])\n",
    "    merged[\"route_encoded\"] = LabelEncoder().fit_transform(merged[\"Route\"])\n",
    "\n",
    "    X = merged[[\"route_encoded\", \"Volume\", \"weather_encoded\"]].values\n",
    "    y = merged[\"DisruptionFlag\"].astype(float).values.reshape(-1, 1)\n",
    "\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    model = DisruptionPredictor()\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X_tensor)\n",
    "        loss = loss_fn(out, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(model_dir, \"task4_disruption_model.pt\"))\n",
    "    print(\"✅ Task 4 saved\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Task 4 error:\", e)\n",
    "\n",
    "# --------------------------\n",
    "# TASK 5 — Time Series LSTM\n",
    "# --------------------------\n",
    "try:\n",
    "    df = pd.read_csv(\"shipment_logistics_dataset.csv\", parse_dates=[\"ActualShipDate\", \"ActualDeliveryDate\"])\n",
    "    df[\"DeliveryTimeDays\"] = (df[\"ActualDeliveryDate\"] - df[\"ActualShipDate\"]).dt.days\n",
    "    df = df[[\"ActualShipDate\", \"DeliveryTimeDays\"]].dropna()\n",
    "    daily_df = df.groupby(\"ActualShipDate\").mean().reset_index()\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    series = scaler.fit_transform(daily_df[[\"DeliveryTimeDays\"]])\n",
    "\n",
    "    def create_sequences(data, window):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - window):\n",
    "            X.append(data[i:i + window])\n",
    "            y.append(data[i + window])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    X, y = create_sequences(series, 5)\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='relu', input_shape=(X.shape[1], X.shape[2])),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X, y, epochs=5, verbose=0)\n",
    "\n",
    "    save_model(model, os.path.join(model_dir, \"task5_timeseries_lstm.keras\"))\n",
    "    print(\"✅ Task 5 saved\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Task 5 error:\", e)\n",
    "\n",
    "# --------------------------\n",
    "# TASK 6 — CNN Package Model\n",
    "# --------------------------\n",
    "try:\n",
    "    # Load dummy images from previous setup\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    base_dir = \"package_images\"\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "    train_gen = datagen.flow_from_directory(base_dir, target_size=(150,150), batch_size=32, subset='training')\n",
    "    val_gen = datagen.flow_from_directory(base_dir, target_size=(150,150), batch_size=32, subset='validation')\n",
    "\n",
    "    cnn_model = Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    cnn_model.fit(train_gen, epochs=1, validation_data=val_gen)\n",
    "    save_model(cnn_model, os.path.join(model_dir, \"task6_package_cnn.keras\"))\n",
    "    print(\"✅ Task 6 saved\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Task 6 error:\", e)\n",
    "\n",
    "# --------------------------\n",
    "# TASK 7 — Autoencoder\n",
    "# --------------------------\n",
    "try:\n",
    "    df = pd.read_csv(\"task7_anomaly_dataset.csv\")\n",
    "    dims = df[\"Dimensions\"].str.split(\"x\", expand=True).astype(float)\n",
    "    df[\"DimVolume\"] = dims[0] * dims[1] * dims[2]\n",
    "    X = df[[\"Weight\", \"Volume\", \"DimVolume\"]].values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    input_layer = Input(shape=(3,))\n",
    "    encoded = Dense(2, activation=\"relu\")(input_layer)\n",
    "    decoded = Dense(3, activation=\"sigmoid\")(encoded)\n",
    "    autoencoder = tf.keras.Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    autoencoder.fit(X_scaled, X_scaled, epochs=5, verbose=0)\n",
    "\n",
    "    autoencoder.save(os.path.join(model_dir, \"task7_anomaly_autoencoder.keras\"))\n",
    "    print(\"✅ Task 7 saved\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Task 7 error:\", e)\n",
    "\n",
    "# --------------------------\n",
    "# TASK 8 — Maintenance Classifier\n",
    "# --------------------------\n",
    "try:\n",
    "    df = pd.read_csv(\"task8_maintenance_dataset.csv\")\n",
    "    X = df.drop(\"BreakdownFlag\", axis=1)\n",
    "    y = df[\"BreakdownFlag\"]\n",
    "    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "    clf_model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    clf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    clf_model.fit(X_train, y_train, epochs=5, verbose=0)\n",
    "\n",
    "    save_model(clf_model, os.path.join(model_dir, \"task8_maintenance_classifier.keras\"))\n",
    "    print(\"✅ Task 8 saved\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Task 8 error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "928bd016-3657-4f1b-aff8-20fae726ad80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8001 images belonging to 2 classes.\n",
      "Found 1999 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\USER PC\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 1s/step - accuracy: 0.4942 - loss: 0.7585 - val_accuracy: 0.5038 - val_loss: 0.6931\n",
      "Epoch 2/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 710ms/step - accuracy: 0.4982 - loss: 0.6932 - val_accuracy: 0.5038 - val_loss: 0.6931\n",
      "Epoch 3/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 707ms/step - accuracy: 0.5080 - loss: 0.6931 - val_accuracy: 0.5038 - val_loss: 0.6931\n",
      "Epoch 4/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 675ms/step - accuracy: 0.4908 - loss: 0.6932 - val_accuracy: 0.5038 - val_loss: 0.6931\n",
      "Epoch 5/5\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 716ms/step - accuracy: 0.5117 - loss: 0.6930 - val_accuracy: 0.5038 - val_loss: 0.6931\n",
      "✅ Task 6 model saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# STEP 1: Rebuild Label File (if missing)\n",
    "df = pd.read_csv(\"shipment_logistics_dataset.csv\")\n",
    "\n",
    "# Ensure labels are only \"OK\" and \"Damaged\"\n",
    "df = df[df[\"ImageLabel\"].isin([\"OK\", \"Damaged\"])]\n",
    "df[[\"PackageImage\", \"ImageLabel\"]].to_csv(\"task6_image_labels_dataset.csv\", index=False)\n",
    "\n",
    "# STEP 2: Prepare Image Directory\n",
    "label_df = pd.read_csv(\"task6_image_labels_dataset.csv\")\n",
    "base_dir = \"package_images\"\n",
    "\n",
    "if os.path.exists(base_dir):\n",
    "    shutil.rmtree(base_dir)\n",
    "\n",
    "for label in [\"OK\", \"Damaged\"]:\n",
    "    os.makedirs(os.path.join(base_dir, label), exist_ok=True)\n",
    "\n",
    "# Generate dummy images\n",
    "for _, row in label_df.iterrows():\n",
    "    label = row[\"ImageLabel\"]\n",
    "    name = row[\"PackageImage\"]\n",
    "    img_path = os.path.join(base_dir, label, name)\n",
    "    if not os.path.exists(img_path):\n",
    "        img = Image.fromarray(np.random.randint(0, 255, (150, 150, 3), dtype=np.uint8))\n",
    "        img.save(img_path)\n",
    "\n",
    "# STEP 3: Data Generator (✅ with class_mode='binary')\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",  # ✅ binary for sigmoid output\n",
    "    subset=\"training\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",  # ✅ must match model's final layer\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# STEP 4: CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # ✅ binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# STEP 5: Train\n",
    "model.fit(train_gen, epochs=5, validation_data=val_gen)\n",
    "\n",
    "# STEP 6: Save\n",
    "model.save(\"models/task6_package_cnn.keras\")\n",
    "print(\"✅ Task 6 model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5ad638ce-f4b5-4c5f-8614-b82d3102ee3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER PC\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "42662cf9-2308-4273-b891-97a0d5291480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_dir = os.path.join(\"C:\", \"Users\", \"USER PC\", \"Downloads\", \"heavyhaul\")\n",
    "# model_path = os.path.join(project_dir, \"models\", \"task6_package_cnn.keras\")\n",
    "# model.save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d8168718-c9b0-42ba-9f1d-d6e2f0b0493f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task 6 model saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "project_dir = os.path.join(\"C:\\\\\", \"Users\", \"USER PC\", \"Downloads\", \"heavyhaul\", \"models\")\n",
    "os.makedirs(project_dir, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "model_path = os.path.join(project_dir, \"task6_package_cnn.keras\")\n",
    "model.save(model_path)\n",
    "print(\"✅ Task 6 model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2698e023-9381-4951-8ce4-b5667c5f3386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: task5_timeseries_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the main dataset\n",
    "df = pd.read_csv(\"shipment_logistics_dataset.csv\", parse_dates=[\"ActualShipDate\", \"ActualDeliveryDate\"])\n",
    "df[\"DeliveryTimeDays\"] = (df[\"ActualDeliveryDate\"] - df[\"ActualShipDate\"]).dt.days\n",
    "\n",
    "# Aggregate to daily averages\n",
    "daily_df = df[[\"ActualShipDate\", \"DeliveryTimeDays\"]].dropna()\n",
    "daily_avg = daily_df.groupby(\"ActualShipDate\").mean().reset_index()\n",
    "daily_avg.rename(columns={\"DeliveryTimeDays\": \"AvgDeliveryTime\"}, inplace=True)\n",
    "\n",
    "# Save the dataset\n",
    "daily_avg.to_csv(\"task5_timeseries_dataset.csv\", index=False)\n",
    "print(\"✅ Saved: task5_timeseries_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2d80a820-1005-4f8c-9e7b-f9440a311df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: task5_timeseries_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the full dataset\n",
    "df = pd.read_csv(\"shipment_logistics_dataset.csv\", parse_dates=[\"ActualShipDate\", \"ActualDeliveryDate\"])\n",
    "\n",
    "# Calculate delivery duration\n",
    "df[\"DeliveryTimeDays\"] = (df[\"ActualDeliveryDate\"] - df[\"ActualShipDate\"]).dt.days\n",
    "\n",
    "# Group by ship date to get daily average delivery time\n",
    "daily_avg = df[[\"ActualShipDate\", \"DeliveryTimeDays\"]].dropna()\n",
    "daily_avg = daily_avg.groupby(\"ActualShipDate\").mean().reset_index()\n",
    "daily_avg.rename(columns={\"DeliveryTimeDays\": \"AvgDeliveryTime\"}, inplace=True)\n",
    "\n",
    "# Save to file (make sure it's in the same directory as your app script)\n",
    "daily_avg.to_csv(\"task5_timeseries_dataset.csv\", index=False)\n",
    "print(\"✅ Saved: task5_timeseries_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "775104d2-5307-4fa7-86f5-258b22c4b260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: models/task7_scaler.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load dataset\n",
    "df7 = pd.read_csv(\"task7_anomaly_dataset.csv\")\n",
    "\n",
    "# Recompute DimVolume\n",
    "dims = df7[\"Dimensions\"].str.split(\"x\", expand=True).astype(float)\n",
    "dims.columns = [\"L\", \"W\", \"H\"]\n",
    "df7[\"DimVolume\"] = dims[\"L\"] * dims[\"W\"] * dims[\"H\"]\n",
    "\n",
    "# Select features\n",
    "X7 = df7[[\"Weight\", \"Volume\", \"DimVolume\"]].values\n",
    "\n",
    "# Fit scaler\n",
    "scaler7 = StandardScaler()\n",
    "scaler7.fit(X7)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Save scaler\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(scaler7, \"models/task7_scaler.joblib\")\n",
    "print(\"✅ Saved: models/task7_scaler.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7980c6fd-a8ff-4551-a765-478479adfbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset\n",
    "# df8 = pd.read_csv(\"task8_maintenance_dataset.csv\")\n",
    "\n",
    "# # Drop target column\n",
    "# X8 = df8.drop(\"BreakdownFlag\", axis=1)\n",
    "\n",
    "# # Fit scaler\n",
    "# scaler8 = StandardScaler()\n",
    "# scaler8.fit(X8)\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "# # Save scaler\n",
    "# joblib.dump(scaler8, \"models/task8_scaler.joblib\")\n",
    "# print(\"✅ Saved: models/task8_scaler.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3fffd276-3a99-4dfb-b218-e351874e374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER PC\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4f3cdf66-526a-4478-94cb-7cd5ff485c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task 8 scaler saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your Task 8 dataset\n",
    "df = pd.read_csv(\"task8_maintenance_dataset.csv\")\n",
    "\n",
    "# Features and target\n",
    "X8 = df.drop(\"BreakdownFlag\", axis=1)\n",
    "y8 = df[\"BreakdownFlag\"]\n",
    "\n",
    "# Split the data\n",
    "X_train8, X_test8, y_train8, y_test8 = train_test_split(X8, y8, test_size=0.2, stratify=y8, random_state=42)\n",
    "\n",
    "# Fit scaler on Task 8 data\n",
    "scaler8 = StandardScaler()\n",
    "X_train_scaled8 = scaler8.fit_transform(X_train8)\n",
    "X_test_scaled8 = scaler8.transform(X_test8)\n",
    "\n",
    "# ✅ Save the Task 8 scaler\n",
    "joblib.dump(scaler8, \"models/task8_scaler.joblib\")\n",
    "print(\"✅ Task 8 scaler saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26aa655-965b-4246-8091-c0209ac99152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
